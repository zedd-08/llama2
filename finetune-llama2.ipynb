{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning llama2 models - from turorial\n",
    "\n",
    "[Tutorial](https://blog.ovhcloud.com/fine-tuning-llama-2-models-using-a-single-gpu-qlora-and-ai-notebooks/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning LLaMA 2 Models using a single GPU, QLoRA and AI Notebooks\n",
    "\n",
    "*This tutorial walks through the process of fine-tuning [LLaMA 2](https://ai.meta.com/llama/) models, providing step-by-step instructions.*\n",
    "\n",
    "*All the code related to this article is available in our dedicated [GitHub repository](https://github.com/ovh/ai-training-examples/blob/main/notebooks/natural-language-processing/llm/miniconda/llama2-fine-tuning/llama_2_finetuning.ipynb).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "On July 18, 2023, [Meta](https://about.meta.com/) released LLaMA 2, the latest version of their **Large Language Model** (LLM).\n",
    "\n",
    "Trained between January 2023 and July 2023 on 2 trillion tokens, these new models outperforms other LLMs on many benchmarks, including reasoning, coding, proficiency, and knowledge tests. This release comes in different flavors, with parameter sizes of **[7B](https://huggingface.co/meta-llama/Llama-2-7b-hf)**, **[13B](https://huggingface.co/meta-llama/Llama-2-13b-hf)** and a mind-blowing **[70B](https://huggingface.co/meta-llama/Llama-2-70b-hf)**. Models are intended for free for both commercial and research use in English.\n",
    "\n",
    "To suit every text generation needed and fine-tune these models, we will use [QLoRA](https://arxiv.org/abs/2305.14314) (Efficient Finetuning of Quantized LLMs), a highly efficient fine-tuning technique that involves quantizing a pretrained LLM to just 4 bits and adding small “Low-Rank Adapters”. This unique approach allows for fine-tuning LLMs **using just a single GPU**! This technique is supported by the [PEFT](https://huggingface.co/docs/peft/) library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Python environment\n",
    "The following libraries are used for this method (`requirements.txt` file):\n",
    "\n",
    "```\n",
    "torch\n",
    "accelerate @ git+https://github.com/huggingface/accelerate.git\n",
    "bitsandbytes\n",
    "datasets==2.13.1\n",
    "transformers @ git+https://github.com/huggingface/transformers.git\n",
    "peft @ git+https://github.com/huggingface/peft.git\n",
    "trl @ git+https://github.com/lvwerra/trl.git\n",
    "scipy\n",
    "```\n",
    "\n",
    "Then install and import the installed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bitsandbytes as bnb\n",
    "from datasets import load_dataset\n",
    "from functools import partial\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, AutoPeftModelForCausalLM\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, BitsAndBytesConfig, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download LLaMA 2 model\n",
    "As mentioned before, LLaMA 2 models come in different flavors which are 7B, 13B, and 70B. Your choice can be influenced by your computational resources. Indeed, larger models require more resources, memory, processing power, and training time.\n",
    "\n",
    "To download the model you have been granted access to, **make sure you are logged in to the Hugging Face model hub**. As mentioned in the requirements step, you need to use the `huggingface-cli` login command.\n",
    "\n",
    "The following function will help us to download the model and its tokenizer. It requires a bitsandbytes configuration that we will define later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "base_dir = '/home/manish/thesis-implementations/quest_generation/llama2/'\n",
    "model_name = 'meta-llama/llama-2-13b-hf'\n",
    "model_dir = 'models'\n",
    "model_path = os.path.join(base_dir, model_dir, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/manish/thesis-implementations/quest_generation/llama2/models/meta-llama/llama-2-13b-hf True\n"
     ]
    }
   ],
   "source": [
    "print(model_path, os.path.exists(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokenizer(model_name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    # Needed for LLaMA tokenizer\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name, bnb_config):\n",
    "    n_gpus = torch.cuda.device_count()\n",
    "    max_memory = f'{12288}MB'\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        use_cache=False,\n",
    "        device_map=\"auto\",\n",
    "        max_memory = {i: max_memory for i in range(n_gpus)},\n",
    "    )\n",
    "\n",
    "    return model, load_tokenizer(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quest Dataset\n",
    "\n",
    "Load the qust dataset for training and create prompts accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "\n",
    "dataset_path = os.path.join(base_dir, 'data')\n",
    "train_file = 'train.jsonl'\n",
    "val_file = 'val.jsonl'\n",
    "data_files = {\n",
    "\t\"train\": train_file, \n",
    "\t\"val\": val_file\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a bitsandbytes configuration and load the model and tokenizer\n",
    "This will allow us to load our LLM in 4 bits. This way, we can divide the used memory by 4 and import the model on smaller devices. We choose to apply bfloat16 compute data type and nested quantization for memory-saving purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bnb_config():\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    )\n",
    "\n",
    "    return bnb_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To leverage the LoRa method, we need to wrap the model as a PeftModel.\n",
    "\n",
    "To do this, we need to implement a [LoRa configuration](https://huggingface.co/docs/peft/conceptual_guides/lora):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_peft_config(modules):\n",
    "    \"\"\"\n",
    "    Create Parameter-Efficient Fine-Tuning config for your model\n",
    "    :param modules: Names of the modules to apply Lora to\n",
    "    \"\"\"\n",
    "    config = LoraConfig(\n",
    "        lora_alpha=16,  # parameter for scaling\n",
    "        lora_dropout=0.1,  # dropout probability for layers\n",
    "        r=64,  # dimension of the updated matrices\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=modules,\n",
    "    )\n",
    "\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cdc0ad69e6844afa991ed2fea78b14a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model from HF with user's token and with bitsandbytes config\n",
    "bnb_config = create_bnb_config()\n",
    "model, tokenizer = load_model(model_path, bnb_config)\n",
    "# tokenizer = load_tokenizer(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN_TYPE = 'no_kg'\n",
    "TRAIN_TYPE = 'text_kg'\n",
    "# TRAIN_TYPE = 'tree_kg'\n",
    "KG_DEPTH = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS_TOKEN = tokenizer.eos_token\n",
    "PAD_TOKEN = tokenizer.pad_token\n",
    "BOS_TOKEN = tokenizer.bos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_game = {\n",
    "    'TESO': 'TESOblivion_KG.gml',\n",
    "    'TESS': 'TESSkyrim_KG.gml',\n",
    "    'TL2': 'Torchlight2_KG.gml',\n",
    "    'MC': 'Minecraft_KG.gml',\n",
    "    'BG1': 'BaldursGate1_KG.gml',\n",
    "    'BG2': 'BaldursGate2_KG.gml'\n",
    "}\n",
    "\n",
    "kg_map = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_base_dir = '/home/manish/thesis-implementations/data/VartinenFormatted/KGs'\n",
    "\n",
    "for gid, gname in map_game.items():\n",
    "    kg_path = os.path.join(kg_base_dir, map_game[gid])\n",
    "    kg = nx.read_gml(kg_path)\n",
    "    kg_map[gid] = kg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_prompt_formats(input):\n",
    "    \"\"\"\n",
    "    Format various fields of the input quest data ('plots', 'kb', 'quest')\n",
    "    Then concatenate them using two newline characters\n",
    "    :param input: input dictionary\n",
    "    \"\"\"\n",
    "\n",
    "    BACKGROUND = \"### Background:\"\n",
    "    PLOTS_KEY = \"### Plots:\"\n",
    "    INTRO_BLURB = \"The quest related to the above information is as follows:\"\n",
    "    QUEST = \"### Quest:\"\n",
    "    END_KEY = \"### End\"\n",
    "    \n",
    "\n",
    "    blurb = f\"{INTRO_BLURB}\"  # add intro blurb - model system instruction\n",
    "\n",
    "    # add background - only if knowledge graph as text\n",
    "    background = ''\n",
    "    \n",
    "    # add plots - key plot points\n",
    "    plots_str = '\\n'.join(input['plots'])\n",
    "    plots = f\"{PLOTS_KEY}\\n{plots_str}\"\n",
    "    \n",
    "    if TRAIN_TYPE == 'text_kg':\n",
    "        completed_rels = []\n",
    "        completed_nodes = []\n",
    "        \n",
    "        for kb in input['kbs']:\n",
    "            entity = kb['name']\n",
    "            e_desc = kb['description']\n",
    "            e_type = kb['type']\n",
    "            e_relations = kb['relations']\n",
    "            \n",
    "            background += f'{entity} is a {e_type}. '\n",
    "            \n",
    "            if entity != e_desc:\n",
    "                background+= f'{entity} is {e_desc}. '\n",
    "                \n",
    "            for rel in e_relations:\n",
    "                background += f'{entity} is {rel[0]} {rel[1]}.'\n",
    "                completed_rels.append((entity, rel[1]))\n",
    "            completed_nodes.append(entity)                     \n",
    "            background += '\\n'\n",
    "        \n",
    "        kg = kg_map[input['game']]\n",
    "        all_nodes = kg.nodes(data=True)\n",
    "        for node in all_nodes:\n",
    "            entity = node[0]\n",
    "            if entity.lower() in plots.lower():\n",
    "                edges = list(nx.dfs_edges(kg, source=entity))\n",
    "                for ent1, ent2 in edges:\n",
    "                    if (ent1, ent2) in completed_rels or (ent2, ent1) in completed_rels:\n",
    "                        continue\n",
    "                    e1_type = all_nodes[ent1]['type']\n",
    "                    e1_desc = all_nodes[ent1]['description']\n",
    "                    e2_type = all_nodes[ent2]['type']\n",
    "                    e2_desc = all_nodes[ent2]['description']\n",
    "                    \n",
    "                    if ent1 not in completed_nodes:\n",
    "                        background += f'{ent1} is a {e1_type}. '\n",
    "                        if e1_desc != ent1:\n",
    "                            background += f'{ent1} is {e1_desc}. '\n",
    "                        completed_nodes.append(ent1)\n",
    "                        background += '\\n'\n",
    "                    \n",
    "                    if ent2 not in completed_nodes:\n",
    "                        background += f'{ent1} is a {e2_type}. '\n",
    "                        if e2_desc != ent2:\n",
    "                            background += f'{ent2} is {e2_desc}. '\n",
    "                        completed_nodes.append(ent2)\n",
    "                        background += '\\n'\n",
    "                    \n",
    "                    rel = kg[ent1][ent2]['label']\n",
    "                    if rel == 'connected to':\n",
    "                        background += f'{ent1} is {rel} {ent2}. '\n",
    "                    if rel == 'present in':\n",
    "                        if e1_type == 'location':\n",
    "                            background += f'{ent2} is {rel} {ent2}. '\n",
    "                        else:\n",
    "                            background += f'{ent1} is {rel} {ent2}. '\n",
    "                    if rel == 'held by':\n",
    "                        if e1_type == 'character':\n",
    "                            background += f'{ent2} is {rel} {ent1}. '\n",
    "                        else:\n",
    "                            background += f'{ent1} is {rel} {ent2}. '\n",
    "                    background += '\\n'\n",
    "                    completed_rels.append((ent1, ent2))\n",
    "                        \n",
    "        background = f\"{BACKGROUND}\\n{background}\"\n",
    "    \n",
    "    # add concatenated quest text\n",
    "    quest_str = ''\n",
    "    for k,v in input['quest'].items():\n",
    "        if k == 'description':\n",
    "            continue\n",
    "        if k == 'tasks':\n",
    "            value = '\\n ' + '\\n '.join(np.char.capitalize(v[:-1]))\n",
    "        else:\n",
    "            value = v.capitalize()\n",
    "        quest_str += f'{k.capitalize()}: {value}\\n' \n",
    "    quest = f\"{QUEST}\\n{quest_str}\"  # add quest output\n",
    "    \n",
    "    end = f\"{END_KEY}\"  # add end key\n",
    "    \n",
    "    if TRAIN_TYPE in ['no_kg', 'tree_kg']:\n",
    "        parts = [part for part in [plots, blurb, quest, end] if part]\n",
    "    else:\n",
    "        parts = [part for part in [background, plots, blurb, quest, end] if part]\n",
    "\n",
    "    formatted_prompt = \"\\n\\n\".join(parts)\n",
    "    input['text'] = formatted_prompt\n",
    "\n",
    "    return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py\n",
    "def get_max_length(model):\n",
    "    max_length = None\n",
    "    for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n",
    "        max_length = getattr(model.config, length_setting, None)\n",
    "        if max_length:\n",
    "            print(f\"Found max lenth: {max_length}\")\n",
    "            break\n",
    "    if not max_length:\n",
    "        max_length = 1024\n",
    "        print(f\"Using default max length: {max_length}\")\n",
    "    return max_length\n",
    "\n",
    "\n",
    "def preprocess_batch(batch, tokenizer, max_length):\n",
    "    \"\"\"\n",
    "    Tokenizing a batch\n",
    "    \"\"\"\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "\n",
    "# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py\n",
    "def preprocess_dataset(tokenizer: AutoTokenizer, max_length: int, dataset: str, include_kg: bool = True):\n",
    "    \"\"\"Format & tokenize it so it is ready for training\n",
    "    :param tokenizer (AutoTokenizer): Model Tokenizer\n",
    "    :param max_length (int): Maximum number of tokens to emit from tokenizer\n",
    "    :param include_kg (bool): Whether to include knowledge graph in the prompt\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add prompt to each sample\n",
    "    print(\"Preprocessing dataset...\")\n",
    "    dataset = dataset.map(create_training_prompt_formats)#, batched=True)\n",
    "    \n",
    "    # Apply preprocessing to each batch of the dataset & and remove 'instruction', 'context', 'response', 'category' fields\n",
    "    _preprocessing_function = partial(preprocess_batch, max_length=max_length, tokenizer=tokenizer)\n",
    "    dataset = dataset.map(\n",
    "        _preprocessing_function,\n",
    "        batched=True,\n",
    "        remove_columns=[\"id\", \"game\", \"kbs\", \"plots\", \"quest\"],\n",
    "    )\n",
    "\n",
    "    # Filter out samples that have input_ids exceeding max_length\n",
    "    dataset = dataset.filter(lambda sample: len(sample[\"input_ids\"]) < max_length)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will use the **model tokenizer to process these prompts into tokenized ones**.\n",
    "\n",
    "The goal is to create input sequences of uniform length (which are suitable for fine-tuning the language model because it maximizes efficiency and minimize computational overhead), that must not exceed the model’s maximum token limit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previous function needs the target modules to update the necessary matrices. The following function will get them for our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOURCE https://github.com/artidoro/qlora/blob/main/qlora.py\n",
    "\n",
    "def find_all_linear_names(model):\n",
    "    cls = bnb.nn.Linear4bit #if args.bits == 4 else (bnb.nn.Linear8bitLt if args.bits == 8 else torch.nn.Linear)\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "\n",
    "    if 'lm_head' in lora_module_names:  # needed for 16-bit\n",
    "        lora_module_names.remove('lm_head')\n",
    "    return list(lora_module_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once everything is set up and the base model is prepared, we can use the `print_trainable_parameters()` helper function to see how many trainable parameters are in the model. We expect the LoRa model to have fewer trainable parameters compared to the original one, since we want to perform fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model, use_4bit=False):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        num_params = param.numel()\n",
    "        # if using DS Zero 3 and the weights are initialized empty\n",
    "        if num_params == 0 and hasattr(param, \"ds_numel\"):\n",
    "            num_params = param.ds_numel\n",
    "\n",
    "        all_param += num_params\n",
    "        if param.requires_grad:\n",
    "            trainable_params += num_params\n",
    "    if use_4bit:\n",
    "        trainable_params /= 2\n",
    "    print(\n",
    "        f\"all params: {all_param:,d} || trainable params: {trainable_params:,d} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "Now that everything is ready, we can pre-process our dataset and load our model using the set configurations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/manish/.cache/huggingface/datasets/json/data-6359e290ba54d2fa/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e56cb5db983c4055a753d2d882f4a69f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(dataset_path, data_files=data_files)\n",
    "train_dataset = dataset['train']\n",
    "val_dataset = dataset['val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found max lenth: 4096\n",
      "Preprocessing dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06757d948dfb400c966345f8c97182d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/77 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dae06090d114da592a4659f84805d28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/77 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9043ada43cd044868b4c3794717d959d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/77 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Preprocess dataset\n",
    "\n",
    "max_length = get_max_length(model)\n",
    "train_dataset = preprocess_dataset(tokenizer, max_length, train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_map = None\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, tokenizer, dataset, output_dir):\n",
    "    # Apply preprocessing to the model to prepare it by\n",
    "    # 1 - Enabling gradient checkpointing to reduce memory usage during fine-tuning\n",
    "    model.gradient_checkpointing_enable()\n",
    "\n",
    "    # 2 - Using the prepare_model_for_kbit_training method from PEFT\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "    # Get lora module names\n",
    "    modules = find_all_linear_names(model)\n",
    "\n",
    "    # Create PEFT config for these modules and wrap the model to PEFT\n",
    "    peft_config = create_peft_config(modules)\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    \n",
    "    # Print information about the percentage of trainable parameters\n",
    "    print_trainable_parameters(model)\n",
    "    \n",
    "    # Training parameters\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        train_dataset=dataset,\n",
    "        args=TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            overwrite_output_dir=True,\n",
    "            per_device_train_batch_size=1,\n",
    "            gradient_accumulation_steps=4,\n",
    "            optim=\"paged_adamw_32bit\",\n",
    "            logging_steps=20,\n",
    "            learning_rate=2e-4,\n",
    "            fp16=True,\n",
    "            warmup_steps=10,\n",
    "            max_steps=200,\n",
    "        ),\n",
    "        data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "    )\n",
    "    \n",
    "    model.config.use_cache = False  # re-enable for inference to speed up predictions for similar inputs\n",
    "    \n",
    "    ### SOURCE https://github.com/artidoro/qlora/blob/main/qlora.py\n",
    "    # Verifying the datatypes before training\n",
    "    \n",
    "    dtypes = {}\n",
    "    for _, p in model.named_parameters():\n",
    "        dtype = p.dtype\n",
    "        if dtype not in dtypes: dtypes[dtype] = 0\n",
    "        dtypes[dtype] += p.numel()\n",
    "    total = 0\n",
    "    for k, v in dtypes.items(): total+= v\n",
    "    for k, v in dtypes.items():\n",
    "        print(k, v, v/total)\n",
    "    \n",
    "    # Launch training\n",
    "    print(\"Training...\")\n",
    "    \n",
    "    train_result = trainer.train()\n",
    "    metrics = train_result.metrics\n",
    "    trainer.log_metrics(\"train\", metrics)\n",
    "    trainer.save_metrics(\"train\", metrics)\n",
    "    trainer.save_state()\n",
    "    print(metrics)    \n",
    "    \n",
    "    # Saving model\n",
    "    print(\"Saving last checkpoint of the model...\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    trainer.model.save_pretrained(output_dir)\n",
    "    \n",
    "    # Free memory for merging weights\n",
    "    del model\n",
    "    del trainer\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    \n",
    "output_dir = os.path.join(base_dir, model_dir, 'results', model_name, f'{TRAIN_TYPE}')\n",
    "train(model, tokenizer, train_dataset, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*If you prefer to have a number of epochs (entire training dataset will be passed through the model) instead of a number of training steps (forward and backward passes through the model with one batch of data), you can replace the `max_steps` argument by `num_train_epochs`.*\n",
    "\n",
    "To later load and use the model for inference, we have used the `trainer.model.save_pretrained(output_dir)` function, which saves the fine-tuned model’s weights, configuration, and tokenizer files.\n",
    "\n",
    "Unfortunately, it is possible that the latest weights are not the best. To solve this problem, you can implement a `EarlyStoppingCallback`, from transformers, during your fine-tuning. This will enable you to regularly test your model on the validation set, if you have one, and keep only the best weights."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama2",
   "language": "python",
   "name": "llama2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
