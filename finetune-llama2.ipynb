{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning llama2 models - from turorial\n",
    "\n",
    "[Tutorial](https://blog.ovhcloud.com/fine-tuning-llama-2-models-using-a-single-gpu-qlora-and-ai-notebooks/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning LLaMA 2 Models using a single GPU, QLoRA and AI Notebooks\n",
    "\n",
    "*This tutorial walks through the process of fine-tuning [LLaMA 2](https://ai.meta.com/llama/) models, providing step-by-step instructions.*\n",
    "\n",
    "*All the code related to this article is available in our dedicated [GitHub repository](https://github.com/ovh/ai-training-examples/blob/main/notebooks/natural-language-processing/llm/miniconda/llama2-fine-tuning/llama_2_finetuning.ipynb).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "On July 18, 2023, [Meta](https://about.meta.com/) released LLaMA 2, the latest version of their **Large Language Model** (LLM).\n",
    "\n",
    "Trained between January 2023 and July 2023 on 2 trillion tokens, these new models outperforms other LLMs on many benchmarks, including reasoning, coding, proficiency, and knowledge tests. This release comes in different flavors, with parameter sizes of **[7B](https://huggingface.co/meta-llama/Llama-2-7b-hf)**, **[13B](https://huggingface.co/meta-llama/Llama-2-13b-hf)** and a mind-blowing **[70B](https://huggingface.co/meta-llama/Llama-2-70b-hf)**. Models are intended for free for both commercial and research use in English.\n",
    "\n",
    "To suit every text generation needed and fine-tune these models, we will use [QLoRA](https://arxiv.org/abs/2305.14314) (Efficient Finetuning of Quantized LLMs), a highly efficient fine-tuning technique that involves quantizing a pretrained LLM to just 4 bits and adding small “Low-Rank Adapters”. This unique approach allows for fine-tuning LLMs **using just a single GPU**! This technique is supported by the [PEFT](https://huggingface.co/docs/peft/) library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Python environment\n",
    "The following libraries are used for this method (`requirements.txt` file):\n",
    "\n",
    "```\n",
    "torch\n",
    "accelerate @ git+https://github.com/huggingface/accelerate.git\n",
    "bitsandbytes\n",
    "datasets==2.13.1\n",
    "transformers @ git+https://github.com/huggingface/transformers.git\n",
    "peft @ git+https://github.com/huggingface/peft.git\n",
    "trl @ git+https://github.com/lvwerra/trl.git\n",
    "scipy\n",
    "```\n",
    "\n",
    "Then install and import the installed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bitsandbytes as bnb\n",
    "from datasets import load_dataset\n",
    "from functools import partial\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, AutoPeftModelForCausalLM\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, BitsAndBytesConfig, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download LLaMA 2 model\n",
    "As mentioned before, LLaMA 2 models come in different flavors which are 7B, 13B, and 70B. Your choice can be influenced by your computational resources. Indeed, larger models require more resources, memory, processing power, and training time.\n",
    "\n",
    "To download the model you have been granted access to, **make sure you are logged in to the Hugging Face model hub**. As mentioned in the requirements step, you need to use the `huggingface-cli` login command.\n",
    "\n",
    "The following function will help us to download the model and its tokenizer. It requires a bitsandbytes configuration that we will define later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "base_dir = '/home/manish/thesis-implementations/quest_generation/llama2/'\n",
    "model_name = 'meta-llama/llama-2-13b-hf'\n",
    "model_dir = 'models'\n",
    "model_path = os.path.join(base_dir, model_dir, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/manish/thesis-implementations/quest_generation/llama2/models/meta-llama/llama-2-13b-hf True\n"
     ]
    }
   ],
   "source": [
    "print(model_path, os.path.exists(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokenizer(model_name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    # Needed for LLaMA tokenizer\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name, bnb_config):\n",
    "    n_gpus = torch.cuda.device_count()\n",
    "    max_memory = f'{12288}MB'\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        use_cache=False,\n",
    "        device_map=\"auto\",\n",
    "        max_memory = {i: max_memory for i in range(n_gpus)},\n",
    "    )\n",
    "\n",
    "    return model, load_tokenizer(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quest Dataset\n",
    "\n",
    "Load the qust dataset for training and create prompts accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "\n",
    "dataset_path = os.path.join(base_dir, 'data')\n",
    "train_file = 'train.jsonl'\n",
    "val_file = 'val.jsonl'\n",
    "data_files = {\n",
    "\t\"train\": train_file, \n",
    "\t\"val\": val_file\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a bitsandbytes configuration and load the model and tokenizer\n",
    "This will allow us to load our LLM in 4 bits. This way, we can divide the used memory by 4 and import the model on smaller devices. We choose to apply bfloat16 compute data type and nested quantization for memory-saving purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bnb_config():\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    )\n",
    "\n",
    "    return bnb_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To leverage the LoRa method, we need to wrap the model as a PeftModel.\n",
    "\n",
    "To do this, we need to implement a [LoRa configuration](https://huggingface.co/docs/peft/conceptual_guides/lora):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_peft_config(modules):\n",
    "    \"\"\"\n",
    "    Create Parameter-Efficient Fine-Tuning config for your model\n",
    "    :param modules: Names of the modules to apply Lora to\n",
    "    \"\"\"\n",
    "    config = LoraConfig(\n",
    "        lora_alpha=16,  # parameter for scaling\n",
    "        lora_dropout=0.1,  # dropout probability for layers\n",
    "        r=64,  # dimension of the updated matrices\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=modules,\n",
    "    )\n",
    "\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cdc0ad69e6844afa991ed2fea78b14a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model from HF with user's token and with bitsandbytes config\n",
    "bnb_config = create_bnb_config()\n",
    "model, tokenizer = load_model(model_path, bnb_config)\n",
    "# tokenizer = load_tokenizer(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN_TYPE = 'no_kg'\n",
    "TRAIN_TYPE = 'text_kg'\n",
    "# TRAIN_TYPE = 'tree_kg'\n",
    "KG_DEPTH = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS_TOKEN = tokenizer.eos_token\n",
    "PAD_TOKEN = tokenizer.pad_token\n",
    "BOS_TOKEN = tokenizer.bos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_game = {\n",
    "    'TESO': 'TESOblivion_KG.gml',\n",
    "    'TESS': 'TESSkyrim_KG.gml',\n",
    "    'TL2': 'Torchlight2_KG.gml',\n",
    "    'MC': 'Minecraft_KG.gml',\n",
    "    'BG1': 'BaldursGate1_KG.gml',\n",
    "    'BG2': 'BaldursGate2_KG.gml'\n",
    "}\n",
    "\n",
    "kg_map = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_base_dir = '/home/manish/thesis-implementations/data/VartinenFormatted/KGs'\n",
    "\n",
    "for gid, gname in map_game.items():\n",
    "    kg_path = os.path.join(kg_base_dir, map_game[gid])\n",
    "    kg = nx.read_gml(kg_path)\n",
    "    kg_map[gid] = kg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_prompt_formats(input):\n",
    "    \"\"\"\n",
    "    Format various fields of the input quest data ('plots', 'kb', 'quest')\n",
    "    Then concatenate them using two newline characters\n",
    "    :param input: input dictionary\n",
    "    \"\"\"\n",
    "\n",
    "    BACKGROUND = \"### Background:\"\n",
    "    PLOTS_KEY = \"### Plots:\"\n",
    "    INTRO_BLURB = \"The quest related to the above information is as follows:\"\n",
    "    QUEST = \"### Quest:\"\n",
    "    END_KEY = \"### End\"\n",
    "    \n",
    "\n",
    "    blurb = f\"{INTRO_BLURB}\"  # add intro blurb - model system instruction\n",
    "\n",
    "    # add background - only if knowledge graph as text\n",
    "    background = ''\n",
    "    \n",
    "    # add plots - key plot points\n",
    "    plots_str = '\\n'.join(input['plots'])\n",
    "    plots = f\"{PLOTS_KEY}\\n{plots_str}\"\n",
    "    \n",
    "    if TRAIN_TYPE == 'text_kg':\n",
    "        completed_rels = []\n",
    "        completed_nodes = []\n",
    "        \n",
    "        for kb in input['kbs']:\n",
    "            entity = kb['name']\n",
    "            e_desc = kb['description']\n",
    "            e_type = kb['type']\n",
    "            e_relations = kb['relations']\n",
    "            \n",
    "            background += f'{entity} is a {e_type}. '\n",
    "            \n",
    "            if entity != e_desc:\n",
    "                background+= f'{entity} is {e_desc}. '\n",
    "                \n",
    "            for rel in e_relations:\n",
    "                background += f'{entity} is {rel[0]} {rel[1]}.'\n",
    "                completed_rels.append((entity, rel[1]))\n",
    "            completed_nodes.append(entity)                     \n",
    "            background += '\\n'\n",
    "        \n",
    "        kg = kg_map[input['game']]\n",
    "        all_nodes = kg.nodes(data=True)\n",
    "        for node in all_nodes:\n",
    "            entity = node[0]\n",
    "            if entity.lower() in plots.lower():\n",
    "                edges = list(nx.dfs_edges(kg, source=entity))\n",
    "                for ent1, ent2 in edges:\n",
    "                    if (ent1, ent2) in completed_rels or (ent2, ent1) in completed_rels:\n",
    "                        continue\n",
    "                    e1_type = all_nodes[ent1]['type']\n",
    "                    e1_desc = all_nodes[ent1]['description']\n",
    "                    e2_type = all_nodes[ent2]['type']\n",
    "                    e2_desc = all_nodes[ent2]['description']\n",
    "                    \n",
    "                    if ent1 not in completed_nodes:\n",
    "                        background += f'{ent1} is a {e1_type}. '\n",
    "                        if e1_desc != ent1:\n",
    "                            background += f'{ent1} is {e1_desc}. '\n",
    "                        completed_nodes.append(ent1)\n",
    "                        background += '\\n'\n",
    "                    \n",
    "                    if ent2 not in completed_nodes:\n",
    "                        background += f'{ent1} is a {e2_type}. '\n",
    "                        if e2_desc != ent2:\n",
    "                            background += f'{ent2} is {e2_desc}. '\n",
    "                        completed_nodes.append(ent2)\n",
    "                        background += '\\n'\n",
    "                    \n",
    "                    rel = kg[ent1][ent2]['label']\n",
    "                    if rel == 'connected to':\n",
    "                        background += f'{ent1} is {rel} {ent2}. '\n",
    "                    if rel == 'present in':\n",
    "                        if e1_type == 'location':\n",
    "                            background += f'{ent2} is {rel} {ent2}. '\n",
    "                        else:\n",
    "                            background += f'{ent1} is {rel} {ent2}. '\n",
    "                    if rel == 'held by':\n",
    "                        if e1_type == 'character':\n",
    "                            background += f'{ent2} is {rel} {ent1}. '\n",
    "                        else:\n",
    "                            background += f'{ent1} is {rel} {ent2}. '\n",
    "                    background += '\\n'\n",
    "                    completed_rels.append((ent1, ent2))\n",
    "                        \n",
    "        background = f\"{BACKGROUND}\\n{background}\"\n",
    "    \n",
    "    # add concatenated quest text\n",
    "    quest_str = ''\n",
    "    for k,v in input['quest'].items():\n",
    "        if k == 'description':\n",
    "            continue\n",
    "        if k == 'tasks':\n",
    "            value = '\\n ' + '\\n '.join(np.char.capitalize(v[:-1]))\n",
    "        else:\n",
    "            value = v.capitalize()\n",
    "        quest_str += f'{k.capitalize()}: {value}\\n' \n",
    "    quest = f\"{QUEST}\\n{quest_str}\"  # add quest output\n",
    "    \n",
    "    end = f\"{END_KEY}\"  # add end key\n",
    "    \n",
    "    if TRAIN_TYPE in ['no_kg', 'tree_kg']:\n",
    "        parts = [part for part in [plots, blurb, quest, end] if part]\n",
    "    else:\n",
    "        parts = [part for part in [background, plots, blurb, quest, end] if part]\n",
    "\n",
    "    formatted_prompt = \"\\n\\n\".join(parts)\n",
    "    input['text'] = formatted_prompt\n",
    "\n",
    "    return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO Direct relations, missed connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_generation_prompt_formats(input):\n",
    "    \"\"\"\n",
    "    Format various fields of the input quest data ('plots', 'kb', 'quest')\n",
    "    Then concatenate them using two newline characters\n",
    "    :param input: input dictionary\n",
    "    \"\"\"\n",
    "\n",
    "    BACKGROUND = \"### Background:\"\n",
    "    PLOTS_KEY = \"### Plots:\"\n",
    "    INTRO_BLURB = \"The quest related to the above information is as follows:\"\n",
    "    QUEST = \"### Quest:\"\n",
    "    END_KEY = \"### End\"\n",
    "    \n",
    "\n",
    "    blurb = f\"{INTRO_BLURB}\"  # add intro blurb - model system instruction\n",
    "\n",
    "    # add background - only if knowledge graph as text\n",
    "    # create sentence tree from plots using kg - only if knowledge graph as tree\n",
    "    background = ''\n",
    "    plots_str = '\\n'.join(input['plots'])\n",
    "    plots = f\"{PLOTS_KEY}\\n{plots_str}\"  # add plots - key plot points\n",
    "    \n",
    "    if TRAIN_TYPE == 'text_kg':\n",
    "        completed_rels = []\n",
    "        completed_nodes = []\n",
    "        \n",
    "        for kb in input['kbs']:\n",
    "            entity = kb['name']\n",
    "            e_desc = kb['description']\n",
    "            e_type = kb['type']\n",
    "            e_relations = kb['relations']\n",
    "            \n",
    "            background += f'{entity} is a {e_type}. '\n",
    "            \n",
    "            # add depth of information from KG\n",
    "            # KG_DEPTH <= 0 will add all information\n",
    "            # KG_DEPTH = 1 will only add description\n",
    "            # KG_DEPTH > 1 will randomly add (KG_DEPTH - 1) number\n",
    "            # of relations and the description\n",
    "            if KG_DEPTH > 0:\n",
    "                if entity != e_desc:\n",
    "                    background+= f'{entity} is {e_desc}. '\n",
    "                depth = KG_DEPTH - 1\n",
    "                if depth > len(e_relations):\n",
    "                    depth = len(e_relations)\n",
    "                relations = random.sample(e_relations, depth)\n",
    "            else:\n",
    "                relations = e_relations\n",
    "                \n",
    "            for rel in relations:\n",
    "                background += f'{entity} is {rel[0]} {rel[1]}. '\n",
    "                completed_rels.append((entity, rel[1]))\n",
    "            completed_nodes.append(entity)                     \n",
    "            background += '\\n'\n",
    "        \n",
    "        if KG_DEPTH > 0:\n",
    "            kg = kg_map[input['game']]        \n",
    "            all_nodes = kg.nodes(data=True)\n",
    "            for node in all_nodes:\n",
    "                entity = node[0]\n",
    "                if entity.lower() in plots.lower():\n",
    "                    edges = list(nx.dfs_edges(kg, source=entity, depth_limit=KG_DEPTH))\n",
    "                    for ent1, ent2 in edges:\n",
    "                        if (ent1, ent2) in completed_rels or (ent2, ent1) in completed_rels:\n",
    "                            continue\n",
    "                        e1_type = all_nodes[ent1]['type']\n",
    "                        e1_desc = all_nodes[ent1]['description']\n",
    "                        e2_type = all_nodes[ent2]['type']\n",
    "                        e2_desc = all_nodes[ent2]['description']\n",
    "                        \n",
    "                        if ent1 not in completed_nodes:\n",
    "                            background += f'{ent1} is a {e1_type}. '\n",
    "                            if e1_desc != ent1:\n",
    "                                background += f'{ent1} is {e1_desc}. '\n",
    "                            completed_nodes.append(ent1)\n",
    "                            background += '\\n'\n",
    "                        \n",
    "                        if ent2 not in completed_nodes:\n",
    "                            background += f'{ent2} is a {e2_type}. '\n",
    "                            if e2_desc != ent2:\n",
    "                                background += f'{ent2} is {e2_desc}. '\n",
    "                            completed_nodes.append(ent2)\n",
    "                            background += '\\n'\n",
    "                        \n",
    "                        rel = kg[ent1][ent2]['label']\n",
    "                        if rel == 'connected to':\n",
    "                            background += f'{ent1} is {rel} {ent2}. '\n",
    "                        if rel == 'present in':\n",
    "                            if e1_type == 'location':\n",
    "                                background += f'{ent2} is {rel} {ent2}. '\n",
    "                            else:\n",
    "                                background += f'{ent1} is {rel} {ent2}. '\n",
    "                        if rel == 'held by':\n",
    "                            if e1_type == 'character':\n",
    "                                background += f'{ent2} is {rel} {ent1}. '\n",
    "                            else:\n",
    "                                background += f'{ent1} is {rel} {ent2}. '\n",
    "                        background += '\\n'\n",
    "                        completed_rels.append((ent1, ent2))\n",
    "                \n",
    "        background = f\"{BACKGROUND}\\n{background}\"\n",
    "        \n",
    "    # add concatenated quest text\n",
    "    quest_str = ''\n",
    "    for k,v in input['quest'].items():\n",
    "        if k == 'description':\n",
    "            continue\n",
    "        if k == 'tasks':\n",
    "            value = '\\n ' + '\\n '.join(np.char.capitalize(v[:-1]))\n",
    "        else:\n",
    "            value = v.capitalize()\n",
    "        quest_str += f'{k.capitalize()}: {value}\\n' \n",
    "    quest = f\"{QUEST}\\n{quest_str}\"  # add quest output\n",
    "    \n",
    "    end = f\"{END_KEY}\"  # add end key\n",
    "    \n",
    "    if TRAIN_TYPE in ['no_kg', 'tree_kg']:\n",
    "        parts_p = [part for part in [plots, blurb] if part]\n",
    "    else:\n",
    "        parts_p = [part for part in [background, plots, blurb] if part]\n",
    "\n",
    "    parts_o = [part for part in [quest, end] if part]\n",
    "    \n",
    "    formatted_prompt = \"\\n\\n\".join(parts_p)\n",
    "    formatted_output = \"\\n\\n\".join(parts_o)\n",
    "    input['text'] = formatted_prompt\n",
    "    input['output'] = formatted_output\n",
    "\n",
    "    return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Background:\n",
      "Martin is a character. Martin is the player's friend. \n",
      "Jauffre is a character. Jauffre is the male Grandmaster of the Blades. \n",
      "Tiber Septim is a character. Tiber Septim is the legendary male Emperor who ascended to divinity. \n",
      "Imperial City is a location. Imperial City is a large city. \n",
      "Martin is present in Imperial City. \n",
      "Skingrad is a location. Skingrad is a city. \n",
      "Imperial City is connected to Skingrad. \n",
      "Rosethorn Hall is a location. Rosethorn Hall is the player's manor. \n",
      "Imperial City is connected to Rosethorn Hall. \n",
      "Arcane University is a location. Arcane University is the headquarters of the Mages Guild. \n",
      "Imperial City is connected to Arcane University. \n",
      "Lonely Suitor Lodge is a location. Lonely Suitor Lodge is an inn. \n",
      "Imperial City is connected to Lonely Suitor Lodge. \n",
      "Aleron Loche is a character. Aleron Loche is Ursanne's missing husband who developed a gambling addiction. \n",
      "Aleron Loche is present in Aleron Loche. \n",
      "Vinicia Melissaeia is a character. Vinicia Melissaeia is Office of Imperial Commerce. \n",
      "Vinicia Melissaeia is present in Vinicia Melissaeia. \n",
      "Three Brothers Trade Goods is a location. Three Brothers Trade Goods is a store in the Market District. \n",
      "Imperial City is connected to Three Brothers Trade Goods. \n",
      "Sir Juncan is a character. Sir Juncan is a ghost of a knight. \n",
      "Sir Juncan is present in Sir Juncan. \n",
      "Shrine of Kynareth is a location. Shrine of Kynareth is west of the Imperial City, on the edge of the Great Forest. \n",
      "Imperial City is connected to Shrine of Kynareth. \n",
      "Marie Elena is a location. Marie Elena is Gaston Tussaud's ship. \n",
      "Imperial City is connected to Marie Elena. \n",
      "Rona is a character. Rona is Janus Hassildor's wife. \n",
      "Rona is present in Rona. \n",
      "Wellspring Cave is a location. Wellspring Cave is a cave. \n",
      "Imperial City is connected to Wellspring Cave. \n",
      "Bruma is a location. Bruma is a city in Cyrodiil. \n",
      "Imperial City is connected to Bruma. \n",
      "Slythe Seringi is a character. Slythe Seringi is a worried man, giving quest via \"Slythe's Journal\". \n",
      "Slythe Seringi is present in Slythe Seringi. \n",
      "Sandstone Cavern is a location. Sandstone Cavern is the offering place of the Sunken One. \n",
      "Imperial City is connected to Sandstone Cavern. \n",
      "S'Drassa is a character. S'Drassa is an evoker and alchemist, and a member of the Mages Guild. \n",
      "S'Drassa is present in S'Drassa. \n",
      "Umbacano is a character. Umbacano is a collector of Ayleid artifacts. \n",
      "Umbacano is present in Umbacano. \n",
      "Nenalata is a location. Nenalata is an ancient Ayleid city. \n",
      "Imperial City is connected to Nenalata. \n",
      "Imperial Prison is a location. Imperial Prison is a prison where the player was imprisoned in the past. \n",
      "Imperial City is connected to Imperial Prison. \n",
      "Daravyn the Gray is a character. Daravyn the Gray is a thief hiding from the guards, giving quest via \"Long Forgotten Note\". \n",
      "Daravyn the Gray is present in Daravyn the Gray. \n",
      "Delgariun is a character. Delgariun is a man who betrayed Daravyn the Gray. \n",
      "Delgariun is present in Delgariun. \n",
      "Leyawiin is a location. Leyawiin is a city. \n",
      "Imperial City is connected to Leyawiin. \n",
      "Seridur is a character. Seridur is an elderly vampire hunter, and the leader of the Order of the Virtuous Blood. \n",
      "Seridur is present in Seridur. \n",
      "Fort Ontus is a location. Fort Ontus is a fort. \n",
      "Imperial City is connected to Fort Ontus. \n",
      "King of Worms is a character. King of Worms is the male master of the necromancers. \n",
      "King of Worms is present in King of Worms. \n",
      "Ocheeva is a character. Ocheeva is the player's superior in the Dark Brotherhood. \n",
      "Ocheeva is present in Ocheeva. \n",
      "Faelian is a character. Faelian is a male high Elf, who fancies long walks, living somewhere in the Imperial City. \n",
      "Faelian is present in Faelian. \n",
      "Adamus Phillida is a character. Adamus Phillida is a male Imperial Legion captain, who has dedicated his life to eradicating the Dark Brotherhood. \n",
      "Adamus Phillida is present in Adamus Phillida. \n",
      "Imperial City Waterfront is a location. Imperial City Waterfront is a poor district in the Imperial City. \n",
      "Imperial City is connected to Imperial City Waterfront. \n",
      "Methredhel is a character. Methredhel is a female thief. \n",
      "Methredhel is present in Methredhel. \n",
      "Stonewall Shields is a location. Stonewall Shields is a shop in the Market District of the Imperial City. \n",
      "Imperial City is connected to Stonewall Shields. \n",
      "Armand Christophe is a character. Armand Christophe is the player's superior in the Thieves Guild. \n",
      "Armand Christophe is present in Armand Christophe. \n",
      "Amantius Allectus is a character. Amantius Allectus is a man. \n",
      "Amantius Allectus is present in Amantius Allectus. \n",
      "Gray Fox is a character. Gray Fox is the player's guildmaster in the Thieves Guild. \n",
      "Gray Fox is present in Gray Fox. \n",
      "Amusei is a character. Amusei is a male thief. \n",
      "Amusei is present in Amusei. \n",
      "High Chancellor Ocato is a character. High Chancellor Ocato is the head of the Elder Council. \n",
      "High Chancellor Ocato is present in High Chancellor Ocato. \n",
      "Oblivion is a location. \n",
      "Imperial City is connected to Oblivion. \n",
      "Mankar Camoran is a character. Mankar Camoran is an evil man serving Mehrunes Dagon. \n",
      "Mankar Camoran is present in Mankar Camoran. \n",
      "Nenyond Twyll is a location. Nenyond Twyll is a Necromancer lair in ancient ruins, south of the Imperial City, south of the White Rose River, nearly halfway to the border of Elsweyr. \n",
      "Imperial City is connected to Nenyond Twyll. \n",
      "Earl of Imbel, Jakben is a character. Earl of Imbel, Jakben is a man , and the last known descendant of Springheel Jak. \n",
      "Earl of Imbel, Jakben is present in Earl of Imbel, Jakben. \n",
      "Savilla's Stone is a object. Savilla's Stone is a stone that reveals secrets to its fielder. \n",
      "Savilla's Stone is present in Savilla's Stone. \n",
      "Springheel Jak is a character. Springheel Jak is a famous long-dead thief. \n",
      "Springheel Jak is present in Springheel Jak. \n",
      "Kvatch is a location. Kvatch is a city. \n",
      "Martin is present in Kvatch. \n",
      "Anvil is a location. Anvil is a city. \n",
      "Kvatch is connected to Anvil. \n",
      "Oleta is a character. Oleta is a restoration master. \n",
      "Oleta is present in Oleta. \n",
      "Chapel of Talos is a location. Chapel of Talos is a large church. \n",
      "Kvatch is connected to Chapel of Talos. \n",
      "Jauffre is present in Jauffre. \n",
      "Savlian Matius is a character. Savlian Matius is a guard captain. \n",
      "Savlian Matius is present in Savlian Matius. \n",
      "Mysterium Xarxes is a object. Mysterium Xarxes is a book of rituals. \n",
      "Mysterium Xarxes is held by Martin. \n",
      "Amulet of Kings is a object. Amulet of Kings is an important amulet. \n",
      "Amulet of Kings is held by Martin. \n",
      "Uriel Septim VII is a character. Uriel Septim VII is the Emperor. \n",
      "Amulet of Kings is held by Uriel Septim VII. \n",
      "Mankar Camoran's Paradise is a location. \n",
      "Martin is present in Mankar Camoran's Paradise. \n",
      "Great Welkynd Stone is a object. Great Welkynd Stone is the third item the player needs for the Mysterium Xarxes ritual, and a rare larger variant of Welkynd Stones valued by mages and occultists. \n",
      "Great Welkynd Stone is held by Martin. \n",
      "Welkynd Stones is a object. Welkynd Stones is magic crystals. \n",
      "Welkynd Stones is held by Martin. \n",
      "Lithnilian is a character. Lithnilian is a researcher of Welkynd Stones. \n",
      "Welkynd Stones is held by Lithnilian. \n",
      "Martina Floria is a character. Martina Floria is Master Enchanter for the Mages Guild. \n",
      "Welkynd Stones is held by Martina Floria. \n",
      "Miscarcand is a location. Miscarcand is the dangerous ruins of an Ayleid city. \n",
      "Martin is present in Miscarcand. \n",
      "Great Sigil Stones is a object. Great Sigil Stones is rare items, the player needs one of these. \n",
      "Great Sigil Stones is held by Martin. \n",
      "\"Modern Heretics\" is a object. \"Modern Heretics\" is a book. \n",
      "\"Modern Heretics\" is held by Martin. \n",
      "Daedric is a object. Daedric is an adjective referring to the Daedra. \n",
      "Daedric is held by Martin. \n",
      "Camoran's Paradise is a location. Camoran's Paradise is the hideout of the player's enemy. \n",
      "Martin is present in Camoran's Paradise. \n",
      "\n",
      "\n",
      "\n",
      "### Plots:\n",
      "Martin needs the blood of a Divine for the ritual to open the portal to Camoran's Paradise in addition to the first item, the blood of a Daedra\n",
      "\n",
      "The quest related to the above information is as follows:\n"
     ]
    }
   ],
   "source": [
    "out = create_generation_prompt_formats(val_dataset[7])\n",
    "print(out['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py\n",
    "def get_max_length(model):\n",
    "    max_length = None\n",
    "    for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n",
    "        max_length = getattr(model.config, length_setting, None)\n",
    "        if max_length:\n",
    "            print(f\"Found max lenth: {max_length}\")\n",
    "            break\n",
    "    if not max_length:\n",
    "        max_length = 1024\n",
    "        print(f\"Using default max length: {max_length}\")\n",
    "    return max_length\n",
    "\n",
    "\n",
    "def preprocess_batch(batch, tokenizer, max_length):\n",
    "    \"\"\"\n",
    "    Tokenizing a batch\n",
    "    \"\"\"\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "\n",
    "# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py\n",
    "def preprocess_dataset(tokenizer: AutoTokenizer, max_length: int, dataset: str, include_kg: bool = True):\n",
    "    \"\"\"Format & tokenize it so it is ready for training\n",
    "    :param tokenizer (AutoTokenizer): Model Tokenizer\n",
    "    :param max_length (int): Maximum number of tokens to emit from tokenizer\n",
    "    :param include_kg (bool): Whether to include knowledge graph in the prompt\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add prompt to each sample\n",
    "    print(\"Preprocessing dataset...\")\n",
    "    dataset = dataset.map(create_training_prompt_formats)#, batched=True)\n",
    "    \n",
    "    # Apply preprocessing to each batch of the dataset & and remove 'instruction', 'context', 'response', 'category' fields\n",
    "    _preprocessing_function = partial(preprocess_batch, max_length=max_length, tokenizer=tokenizer)\n",
    "    dataset = dataset.map(\n",
    "        _preprocessing_function,\n",
    "        batched=True,\n",
    "        remove_columns=[\"id\", \"game\", \"kbs\", \"plots\", \"quest\"],\n",
    "    )\n",
    "\n",
    "    # Filter out samples that have input_ids exceeding max_length\n",
    "    dataset = dataset.filter(lambda sample: len(sample[\"input_ids\"]) < max_length)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py\n",
    "def preprocess_val_dataset(tokenizer: AutoTokenizer, max_length: int, dataset: str, include_kg: bool = True):\n",
    "    \"\"\"Format & tokenize it so it is ready for training\n",
    "    :param tokenizer (AutoTokenizer): Model Tokenizer\n",
    "    :param max_length (int): Maximum number of tokens to emit from tokenizer\n",
    "    :param include_kg (bool): Whether to include knowledge graph in the prompt\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add prompt to each sample\n",
    "    print(\"Preprocessing dataset...\")\n",
    "    dataset = dataset.map(create_generation_prompt_formats)#, batched=True)\n",
    "    \n",
    "    # Apply preprocessing to each batch of the dataset & and remove 'instruction', 'context', 'response', 'category' fields\n",
    "    _preprocessing_function = partial(preprocess_batch, max_length=max_length, tokenizer=tokenizer)\n",
    "    dataset = dataset.map(\n",
    "        _preprocessing_function,\n",
    "        batched=True,\n",
    "        remove_columns=[\"id\", \"game\", \"kbs\", \"plots\", \"quest\"],\n",
    "    )\n",
    "\n",
    "    # Filter out samples that have input_ids exceeding max_length\n",
    "    dataset = dataset.filter(lambda sample: len(sample[\"input_ids\"]) < max_length)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will use the **model tokenizer to process these prompts into tokenized ones**.\n",
    "\n",
    "The goal is to create input sequences of uniform length (which are suitable for fine-tuning the language model because it maximizes efficiency and minimize computational overhead), that must not exceed the model’s maximum token limit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previous function needs the target modules to update the necessary matrices. The following function will get them for our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOURCE https://github.com/artidoro/qlora/blob/main/qlora.py\n",
    "\n",
    "def find_all_linear_names(model):\n",
    "    cls = bnb.nn.Linear4bit #if args.bits == 4 else (bnb.nn.Linear8bitLt if args.bits == 8 else torch.nn.Linear)\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "\n",
    "    if 'lm_head' in lora_module_names:  # needed for 16-bit\n",
    "        lora_module_names.remove('lm_head')\n",
    "    return list(lora_module_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once everything is set up and the base model is prepared, we can use the `print_trainable_parameters()` helper function to see how many trainable parameters are in the model. We expect the LoRa model to have fewer trainable parameters compared to the original one, since we want to perform fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model, use_4bit=False):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        num_params = param.numel()\n",
    "        # if using DS Zero 3 and the weights are initialized empty\n",
    "        if num_params == 0 and hasattr(param, \"ds_numel\"):\n",
    "            num_params = param.ds_numel\n",
    "\n",
    "        all_param += num_params\n",
    "        if param.requires_grad:\n",
    "            trainable_params += num_params\n",
    "    if use_4bit:\n",
    "        trainable_params /= 2\n",
    "    print(\n",
    "        f\"all params: {all_param:,d} || trainable params: {trainable_params:,d} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "Now that everything is ready, we can pre-process our dataset and load our model using the set configurations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/manish/.cache/huggingface/datasets/json/data-6359e290ba54d2fa/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e56cb5db983c4055a753d2d882f4a69f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(dataset_path, data_files=data_files)\n",
    "train_dataset = dataset['train']\n",
    "val_dataset = dataset['val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found max lenth: 4096\n",
      "Preprocessing dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06757d948dfb400c966345f8c97182d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/77 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dae06090d114da592a4659f84805d28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/77 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9043ada43cd044868b4c3794717d959d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/77 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Preprocess dataset\n",
    "\n",
    "max_length = get_max_length(model)\n",
    "# train_dataset = preprocess_dataset(tokenizer, max_length, train_dataset)\n",
    "val_dataset = preprocess_val_dataset(tokenizer, max_length, val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_map = None\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, tokenizer, dataset, output_dir):\n",
    "    # Apply preprocessing to the model to prepare it by\n",
    "    # 1 - Enabling gradient checkpointing to reduce memory usage during fine-tuning\n",
    "    model.gradient_checkpointing_enable()\n",
    "\n",
    "    # 2 - Using the prepare_model_for_kbit_training method from PEFT\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "    # Get lora module names\n",
    "    modules = find_all_linear_names(model)\n",
    "\n",
    "    # Create PEFT config for these modules and wrap the model to PEFT\n",
    "    peft_config = create_peft_config(modules)\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    \n",
    "    # Print information about the percentage of trainable parameters\n",
    "    print_trainable_parameters(model)\n",
    "    \n",
    "    # Training parameters\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        train_dataset=dataset,\n",
    "        args=TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            overwrite_output_dir=True,\n",
    "            per_device_train_batch_size=1,\n",
    "            gradient_accumulation_steps=4,\n",
    "            optim=\"paged_adamw_32bit\",\n",
    "            logging_steps=20,\n",
    "            learning_rate=2e-4,\n",
    "            fp16=True,\n",
    "            warmup_steps=10,\n",
    "            max_steps=200,\n",
    "        ),\n",
    "        data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "    )\n",
    "    \n",
    "    model.config.use_cache = False  # re-enable for inference to speed up predictions for similar inputs\n",
    "    \n",
    "    ### SOURCE https://github.com/artidoro/qlora/blob/main/qlora.py\n",
    "    # Verifying the datatypes before training\n",
    "    \n",
    "    dtypes = {}\n",
    "    for _, p in model.named_parameters():\n",
    "        dtype = p.dtype\n",
    "        if dtype not in dtypes: dtypes[dtype] = 0\n",
    "        dtypes[dtype] += p.numel()\n",
    "    total = 0\n",
    "    for k, v in dtypes.items(): total+= v\n",
    "    for k, v in dtypes.items():\n",
    "        print(k, v, v/total)\n",
    "    \n",
    "    # Launch training\n",
    "    print(\"Training...\")\n",
    "    \n",
    "    train_result = trainer.train()\n",
    "    metrics = train_result.metrics\n",
    "    trainer.log_metrics(\"train\", metrics)\n",
    "    trainer.save_metrics(\"train\", metrics)\n",
    "    trainer.save_state()\n",
    "    print(metrics)    \n",
    "    \n",
    "    # Saving model\n",
    "    print(\"Saving last checkpoint of the model...\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    trainer.model.save_pretrained(output_dir)\n",
    "    \n",
    "    # Free memory for merging weights\n",
    "    del model\n",
    "    del trainer\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    \n",
    "output_dir = os.path.join(base_dir, model_dir, 'results', model_name, f'{TRAIN_TYPE}_{KG_DEPTH}')\n",
    "train(model, tokenizer, train_dataset, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*If you prefer to have a number of epochs (entire training dataset will be passed through the model) instead of a number of training steps (forward and backward passes through the model with one batch of data), you can replace the `max_steps` argument by `num_train_epochs`.*\n",
    "\n",
    "To later load and use the model for inference, we have used the `trainer.model.save_pretrained(output_dir)` function, which saves the fine-tuned model’s weights, configuration, and tokenizer files.\n",
    "\n",
    "Unfortunately, it is possible that the latest weights are not the best. To solve this problem, you can implement a `EarlyStoppingCallback`, from transformers, during your fine-tuning. This will enable you to regularly test your model on the validation set, if you have one, and keep only the best weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge weights\n",
    "Once we have our fine-tuned weights, we can build our fine-tuned model and save it to a new directory, with its associated tokenizer. By performing these steps, we can have a memory-efficient fine-tuned model and tokenizer ready for inference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/manish/thesis-implementations/quest_generation/llama2/models/results/meta-llama/llama-2-13b-hf/text_kg True\n"
     ]
    }
   ],
   "source": [
    "output_dir = os.path.join(base_dir, model_dir, 'results', model_name, f'{TRAIN_TYPE}')\n",
    "print(output_dir, os.path.exists(output_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60618"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del model\n",
    "del tokenizer\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d13b757a64647bbb098075245dce25b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# base_model = model\n",
    "\n",
    "# from peft import PeftModel    \n",
    "def load_pretrained_model(model_path, bnb_config, base_model_path):\n",
    "    n_gpus = torch.cuda.device_count()\n",
    "    max_memory = f'{12288}MB'\n",
    "\n",
    "    model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        max_memory = {i: max_memory for i in range(n_gpus)},\n",
    "    )\n",
    "    \n",
    "    return model, load_tokenizer(base_model_path)\n",
    "\n",
    "bnb_config = create_bnb_config()\n",
    "model, tokenizer = load_pretrained_model(output_dir, bnb_config, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 180,\n",
       " 'game': 'TESO',\n",
       " 'kbs': [{'name': 'Martin',\n",
       "   'description': \"the player's friend\",\n",
       "   'id': 'TESO336',\n",
       "   'relations': [],\n",
       "   'type': 'character'},\n",
       "  {'name': 'Jauffre',\n",
       "   'description': 'the male Grandmaster of the Blades',\n",
       "   'id': 'TESO259',\n",
       "   'relations': [],\n",
       "   'type': 'character'},\n",
       "  {'name': 'Tiber Septim',\n",
       "   'description': 'the legendary male Emperor who ascended to divinity',\n",
       "   'id': 'TESO555',\n",
       "   'relations': [],\n",
       "   'type': 'character'}],\n",
       " 'plots': [\"Martin needs the blood of a Divine for the ritual to open the portal to Camoran's Paradise in addition to the first item, the blood of a Daedra\"],\n",
       " 'quest': {'title': 'Blood of the Divines',\n",
       "  'objective': 'recover the blood of a Divine for Martin',\n",
       "  'tasks': ['talk to Jauffre about obtaining the blood of Tiber Septim',\n",
       "   'recover the blood of a Divine for Martin'],\n",
       "  'description': \"I've figured out another item needed for the ritual to open the portal to Camoran's Paradise. The second item is the counterpart to the first: the blood of a Divine.\\nThis was a terrible puzzle to me. Unlike the Daedra, the gods have no artifacts, and do not physically manifest themselves in our world.\\nHow then to obtain the blood of a god? But Jauffre solved it. The blood of Tiber Septim himself, who became one of the Divines. This is a secret remembered only by the Blades, passed down from one Grandmaster to the next.\\nJauffre should tell it to you himself.\"}}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Background:\n",
      "Martin is a character. Martin is the player's friend. \n",
      "Jauffre is a character. Jauffre is the male Grandmaster of the Blades. \n",
      "Tiber Septim is a character. Tiber Septim is the legendary male Emperor who ascended to divinity. \n",
      "Martin is a location. Imperial City is a large city. \n",
      "Martin is present in Imperial City. \n",
      "Imperial City is a location. Skingrad is a city. \n",
      "Imperial City is connected to Skingrad. \n",
      "Imperial City is a location. Rosethorn Hall is the player's manor. \n",
      "Imperial City is connected to Rosethorn Hall. \n",
      "Imperial City is a location. Arcane University is the headquarters of the Mages Guild. \n",
      "Imperial City is connected to Arcane University. \n",
      "Imperial City is a location. Lonely Suitor Lodge is an inn. \n",
      "Imperial City is connected to Lonely Suitor Lodge. \n",
      "Imperial City is a character. Aleron Loche is Ursanne's missing husband who developed a gambling addiction. \n",
      "Aleron Loche is present in Aleron Loche. \n",
      "Imperial City is a character. Vinicia Melissaeia is Office of Imperial Commerce. \n",
      "Vinicia Melissaeia is present in Vinicia Melissaeia. \n",
      "Imperial City is a location. Three Brothers Trade Goods is a store in the Market District. \n",
      "Imperial City is connected to Three Brothers Trade Goods. \n",
      "Imperial City is a character. Sir Juncan is a ghost of a knight. \n",
      "Sir Juncan is present in Sir Juncan. \n",
      "Imperial City is a location. Shrine of Kynareth is west of the Imperial City, on the edge of the Great Forest. \n",
      "Imperial City is connected to Shrine of Kynareth. \n",
      "Imperial City is a location. Marie Elena is Gaston Tussaud's ship. \n",
      "Imperial City is connected to Marie Elena. \n",
      "Imperial City is a character. Rona is Janus Hassildor's wife. \n",
      "Rona is present in Rona. \n",
      "Imperial City is a location. Wellspring Cave is a cave. \n",
      "Imperial City is connected to Wellspring Cave. \n",
      "Imperial City is a location. Bruma is a city in Cyrodiil. \n",
      "Imperial City is connected to Bruma. \n",
      "Imperial City is a character. Slythe Seringi is a worried man, giving quest via \"Slythe's Journal\". \n",
      "Slythe Seringi is present in Slythe Seringi. \n",
      "Imperial City is a location. Sandstone Cavern is the offering place of the Sunken One. \n",
      "Imperial City is connected to Sandstone Cavern. \n",
      "Imperial City is a character. S'Drassa is an evoker and alchemist, and a member of the Mages Guild. \n",
      "S'Drassa is present in S'Drassa. \n",
      "Imperial City is a character. Umbacano is a collector of Ayleid artifacts. \n",
      "Umbacano is present in Umbacano. \n",
      "Imperial City is a location. Nenalata is an ancient Ayleid city. \n",
      "Imperial City is connected to Nenalata. \n",
      "Imperial City is a location. Imperial Prison is a prison where the player was imprisoned in the past. \n",
      "Imperial City is connected to Imperial Prison. \n",
      "Imperial City is a character. Daravyn the Gray is a thief hiding from the guards, giving quest via \"Long Forgotten Note\". \n",
      "Daravyn the Gray is present in Daravyn the Gray. \n",
      "Imperial City is a character. Delgariun is a man who betrayed Daravyn the Gray. \n",
      "Delgariun is present in Delgariun. \n",
      "Imperial City is a location. Leyawiin is a city. \n",
      "Imperial City is connected to Leyawiin. \n",
      "Imperial City is a character. Seridur is an elderly vampire hunter, and the leader of the Order of the Virtuous Blood. \n",
      "Seridur is present in Seridur. \n",
      "Imperial City is a location. Fort Ontus is a fort. \n",
      "Imperial City is connected to Fort Ontus. \n",
      "Imperial City is a character. King of Worms is the male master of the necromancers. \n",
      "King of Worms is present in King of Worms. \n",
      "Imperial City is a character. Ocheeva is the player's superior in the Dark Brotherhood. \n",
      "Ocheeva is present in Ocheeva. \n",
      "Imperial City is a character. Faelian is a male high Elf, who fancies long walks, living somewhere in the Imperial City. \n",
      "Faelian is present in Faelian. \n",
      "Imperial City is a character. Adamus Phillida is a male Imperial Legion captain, who has dedicated his life to eradicating the Dark Brotherhood. \n",
      "Adamus Phillida is present in Adamus Phillida. \n",
      "Imperial City is a location. Imperial City Waterfront is a poor district in the Imperial City. \n",
      "Imperial City is connected to Imperial City Waterfront. \n",
      "Imperial City is a character. Methredhel is a female thief. \n",
      "Methredhel is present in Methredhel. \n",
      "Imperial City is a location. Stonewall Shields is a shop in the Market District of the Imperial City. \n",
      "Imperial City is connected to Stonewall Shields. \n",
      "Imperial City is a character. Armand Christophe is the player's superior in the Thieves Guild. \n",
      "Armand Christophe is present in Armand Christophe. \n",
      "Imperial City is a character. Amantius Allectus is a man. \n",
      "Amantius Allectus is present in Amantius Allectus. \n",
      "Imperial City is a character. Gray Fox is the player's guildmaster in the Thieves Guild. \n",
      "Gray Fox is present in Gray Fox. \n",
      "Imperial City is a character. Amusei is a male thief. \n",
      "Amusei is present in Amusei. \n",
      "Imperial City is a character. High Chancellor Ocato is the head of the Elder Council. \n",
      "High Chancellor Ocato is present in High Chancellor Ocato. \n",
      "Imperial City is a location. \n",
      "Imperial City is connected to Oblivion. \n",
      "Imperial City is a character. Mankar Camoran is an evil man serving Mehrunes Dagon. \n",
      "Mankar Camoran is present in Mankar Camoran. \n",
      "Imperial City is a location. Nenyond Twyll is a Necromancer lair in ancient ruins, south of the Imperial City, south of the White Rose River, nearly halfway to the border of Elsweyr. \n",
      "Imperial City is connected to Nenyond Twyll. \n",
      "Imperial City is a character. Earl of Imbel, Jakben is a man , and the last known descendant of Springheel Jak. \n",
      "Earl of Imbel, Jakben is present in Earl of Imbel, Jakben. \n",
      "Imperial City is a object. Savilla's Stone is a stone that reveals secrets to its fielder. \n",
      "Savilla's Stone is present in Savilla's Stone. \n",
      "Imperial City is a character. Springheel Jak is a famous long-dead thief. \n",
      "Springheel Jak is present in Springheel Jak. \n",
      "Martin is a location. Kvatch is a city. \n",
      "Martin is present in Kvatch. \n",
      "Kvatch is a location. Anvil is a city. \n",
      "Kvatch is connected to Anvil. \n",
      "Kvatch is a character. Oleta is a restoration master. \n",
      "Oleta is present in Oleta. \n",
      "Kvatch is a location. Chapel of Talos is a large church. \n",
      "Kvatch is connected to Chapel of Talos. \n",
      "Jauffre is present in Jauffre. \n",
      "Kvatch is a character. Savlian Matius is a guard captain. \n",
      "Savlian Matius is present in Savlian Matius. \n",
      "Martin is a object. Mysterium Xarxes is a book of rituals. \n",
      "Mysterium Xarxes is held by Martin. \n",
      "Martin is a object. Amulet of Kings is an important amulet. \n",
      "Amulet of Kings is held by Martin. \n",
      "Amulet of Kings is a character. Uriel Septim VII is the Emperor. \n",
      "Amulet of Kings is held by Uriel Septim VII. \n",
      "Martin is a location. \n",
      "Martin is present in Mankar Camoran's Paradise. \n",
      "Martin is a object. Great Welkynd Stone is the third item the player needs for the Mysterium Xarxes ritual, and a rare larger variant of Welkynd Stones valued by mages and occultists. \n",
      "Great Welkynd Stone is held by Martin. \n",
      "Martin is a object. Welkynd Stones is magic crystals. \n",
      "Welkynd Stones is held by Martin. \n",
      "Welkynd Stones is a character. Lithnilian is a researcher of Welkynd Stones. \n",
      "Welkynd Stones is held by Lithnilian. \n",
      "Welkynd Stones is a character. Martina Floria is Master Enchanter for the Mages Guild. \n",
      "Welkynd Stones is held by Martina Floria. \n",
      "Martin is a location. Miscarcand is the dangerous ruins of an Ayleid city. \n",
      "Martin is present in Miscarcand. \n",
      "Martin is a object. Great Sigil Stones is rare items, the player needs one of these. \n",
      "Great Sigil Stones is held by Martin. \n",
      "Martin is a object. \"Modern Heretics\" is a book. \n",
      "\"Modern Heretics\" is held by Martin. \n",
      "Martin is a object. Daedric is an adjective referring to the Daedra. \n",
      "Daedric is held by Martin. \n",
      "Martin is a location. Camoran's Paradise is the hideout of the player's enemy. \n",
      "Martin is present in Camoran's Paradise. \n",
      "\n",
      "\n",
      "\n",
      "### Plots:\n",
      "Martin needs the blood of a Divine for the ritual to open the portal to Camoran's Paradise in addition to the first item, the blood of a Daedra\n",
      "\n",
      "The quest related to the above information is as follows:\n"
     ]
    }
   ],
   "source": [
    "print(val_dataset[7]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5109"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7cba6914e8346c4977ff98ff0d1465e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2045\n",
      "3727\n",
      "1417\n",
      "2000\n",
      "318\n",
      "6122\n",
      "594\n",
      "7958\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 472.00 MiB. GPU 0 has a total capacty of 15.68 GiB of which 193.44 MiB is free. Process 1846 has 239.88 MiB memory in use. Including non-PyTorch memory, this process has 14.76 GiB memory in use. Of the allocated memory 13.49 GiB is allocated by PyTorch, and 1005.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m/home/manish/thesis-implementations/quest_generation/llama2/finetune-llama2.ipynb Cell 43\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/manish/thesis-implementations/quest_generation/llama2/finetune-llama2.ipynb#X60sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \tdevice \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcuda:0\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/manish/thesis-implementations/quest_generation/llama2/finetune-llama2.ipynb#X60sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \t\u001b[39m# Get answer\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/manish/thesis-implementations/quest_generation/llama2/finetune-llama2.ipynb#X60sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \t\u001b[39m# (Adjust max_new_tokens variable as you wish (maximum number of tokens the model can generate to answer the input))\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/manish/thesis-implementations/quest_generation/llama2/finetune-llama2.ipynb#X60sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \toutputs \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/manish/thesis-implementations/quest_generation/llama2/finetune-llama2.ipynb#X60sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \t\tinput_ids\u001b[39m=\u001b[39;49minp\u001b[39m.\u001b[39;49mto(device), \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/manish/thesis-implementations/quest_generation/llama2/finetune-llama2.ipynb#X60sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \t\tattention_mask\u001b[39m=\u001b[39;49mattn_mask, \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/manish/thesis-implementations/quest_generation/llama2/finetune-llama2.ipynb#X60sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \t\tmax_new_tokens\u001b[39m=\u001b[39;49m\u001b[39m250\u001b[39;49m, \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/manish/thesis-implementations/quest_generation/llama2/finetune-llama2.ipynb#X60sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \t\teos_token_id\u001b[39m=\u001b[39;49mtokenizer(\u001b[39m'\u001b[39;49m\u001b[39m### End\u001b[39;49m\u001b[39m'\u001b[39;49m)[\u001b[39m'\u001b[39;49m\u001b[39minput_ids\u001b[39;49m\u001b[39m'\u001b[39;49m], \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/manish/thesis-implementations/quest_generation/llama2/finetune-llama2.ipynb#X60sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \t\tpad_token_id\u001b[39m=\u001b[39;49mtokenizer\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/manish/thesis-implementations/quest_generation/llama2/finetune-llama2.ipynb#X60sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m   \t)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/manish/thesis-implementations/quest_generation/llama2/finetune-llama2.ipynb#X60sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \t\u001b[39m# Decode output & append to outptu list\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/manish/thesis-implementations/quest_generation/llama2/finetune-llama2.ipynb#X60sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \toutput_text \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mdecode(outputs[\u001b[39m0\u001b[39m], skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)[inp_txt_len \u001b[39m+\u001b[39m \u001b[39m2\u001b[39m:]\n",
      "File \u001b[0;32m~/miniconda3/envs/llama2/lib/python3.10/site-packages/peft/peft_model.py:1022\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.generate\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   1020\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_model\u001b[39m.\u001b[39mgeneration_config \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgeneration_config\n\u001b[1;32m   1021\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1022\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbase_model\u001b[39m.\u001b[39;49mgenerate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1023\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m   1024\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_model\u001b[39m.\u001b[39mprepare_inputs_for_generation \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_model_prepare_inputs_for_generation\n",
      "File \u001b[0;32m~/miniconda3/envs/llama2/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/llama2/lib/python3.10/site-packages/transformers/generation/utils.py:1704\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1696\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1697\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1698\u001b[0m         expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1699\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1700\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1701\u001b[0m     )\n\u001b[1;32m   1703\u001b[0m     \u001b[39m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1704\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msample(\n\u001b[1;32m   1705\u001b[0m         input_ids,\n\u001b[1;32m   1706\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   1707\u001b[0m         logits_warper\u001b[39m=\u001b[39;49mlogits_warper,\n\u001b[1;32m   1708\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   1709\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m   1710\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m   1711\u001b[0m         output_scores\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_scores,\n\u001b[1;32m   1712\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   1713\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1714\u001b[0m         streamer\u001b[39m=\u001b[39;49mstreamer,\n\u001b[1;32m   1715\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1716\u001b[0m     )\n\u001b[1;32m   1718\u001b[0m \u001b[39melif\u001b[39;00m generation_mode \u001b[39m==\u001b[39m GenerationMode\u001b[39m.\u001b[39mBEAM_SEARCH:\n\u001b[1;32m   1719\u001b[0m     \u001b[39m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   1720\u001b[0m     beam_scorer \u001b[39m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   1721\u001b[0m         batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m   1722\u001b[0m         num_beams\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1727\u001b[0m         max_length\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mmax_length,\n\u001b[1;32m   1728\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/llama2/lib/python3.10/site-packages/transformers/generation/utils.py:2786\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2783\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2785\u001b[0m \u001b[39m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2786\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\n\u001b[1;32m   2787\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs,\n\u001b[1;32m   2788\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   2789\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   2790\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   2791\u001b[0m )\n\u001b[1;32m   2793\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2794\u001b[0m     \u001b[39mcontinue\u001b[39;00m  \u001b[39m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama2/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/llama2/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama2/lib/python3.10/site-packages/accelerate/hooks.py:164\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    163\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39;49m_old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/llama2/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:1041\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1038\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m   1040\u001b[0m \u001b[39m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1041\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\n\u001b[1;32m   1042\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m   1043\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1044\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1045\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1046\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1047\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1048\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1049\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1050\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1051\u001b[0m )\n\u001b[1;32m   1053\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1054\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mpretraining_tp \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/llama2/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/llama2/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama2/lib/python3.10/site-packages/accelerate/hooks.py:164\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    163\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39;49m_old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/llama2/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:928\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    924\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    925\u001b[0m         create_custom_forward(decoder_layer), hidden_states, attention_mask, position_ids\n\u001b[1;32m    926\u001b[0m     )\n\u001b[1;32m    927\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 928\u001b[0m     layer_outputs \u001b[39m=\u001b[39m decoder_layer(\n\u001b[1;32m    929\u001b[0m         hidden_states,\n\u001b[1;32m    930\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    931\u001b[0m         position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    932\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m    933\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    934\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    935\u001b[0m         padding_mask\u001b[39m=\u001b[39;49mpadding_mask,\n\u001b[1;32m    936\u001b[0m     )\n\u001b[1;32m    938\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    940\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniconda3/envs/llama2/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/llama2/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama2/lib/python3.10/site-packages/accelerate/hooks.py:164\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    163\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39;49m_old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/llama2/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:640\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, padding_mask)\u001b[0m\n\u001b[1;32m    637\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    639\u001b[0m \u001b[39m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 640\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attn(\n\u001b[1;32m    641\u001b[0m     hidden_states\u001b[39m=\u001b[39;49mhidden_states,\n\u001b[1;32m    642\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    643\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    644\u001b[0m     past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m    645\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    646\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    647\u001b[0m     padding_mask\u001b[39m=\u001b[39;49mpadding_mask,\n\u001b[1;32m    648\u001b[0m )\n\u001b[1;32m    649\u001b[0m hidden_states \u001b[39m=\u001b[39m residual \u001b[39m+\u001b[39m hidden_states\n\u001b[1;32m    651\u001b[0m \u001b[39m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama2/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/llama2/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama2/lib/python3.10/site-packages/accelerate/hooks.py:164\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    163\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39;49m_old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/llama2/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:387\u001b[0m, in \u001b[0;36mLlamaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, padding_mask)\u001b[0m\n\u001b[1;32m    384\u001b[0m     attn_weights \u001b[39m=\u001b[39m attn_weights \u001b[39m+\u001b[39m attention_mask\n\u001b[1;32m    386\u001b[0m \u001b[39m# upcast attention to fp32\u001b[39;00m\n\u001b[0;32m--> 387\u001b[0m attn_weights \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mfunctional\u001b[39m.\u001b[39;49msoftmax(attn_weights, dim\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mfloat32)\u001b[39m.\u001b[39;49mto(query_states\u001b[39m.\u001b[39;49mdtype)\n\u001b[1;32m    388\u001b[0m attn_output \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmatmul(attn_weights, value_states)\n\u001b[1;32m    390\u001b[0m \u001b[39mif\u001b[39;00m attn_output\u001b[39m.\u001b[39msize() \u001b[39m!=\u001b[39m (bsz, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads, q_len, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_dim):\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 472.00 MiB. GPU 0 has a total capacty of 15.68 GiB of which 193.44 MiB is free. Process 1846 has 239.88 MiB memory in use. Including non-PyTorch memory, this process has 14.76 GiB memory in use. Of the allocated memory 13.49 GiB is allocated by PyTorch, and 1005.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "results = []\n",
    "\n",
    "for item in tqdm(val_dataset):\n",
    "\tinp_txt_len = len(item['text'])\n",
    "\tprint(inp_txt_len)\n",
    "\t# Specify input\n",
    "\tinp = torch.tensor([item['input_ids']])\n",
    "\tattn_mask = torch.tensor([item['attention_mask']])\n",
    "\n",
    "\t# Specify device\n",
    "\tdevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\t# Get answer\n",
    "\t# (Adjust max_new_tokens variable as you wish (maximum number of tokens the model can generate to answer the input))\n",
    "\toutputs = model.generate(\n",
    "\t\tinput_ids=inp.to(device), \n",
    "\t\tattention_mask=attn_mask, \n",
    "\t\tmax_new_tokens=250, \n",
    "\t\teos_token_id=tokenizer('### End')['input_ids'], \n",
    "\t\tpad_token_id=tokenizer.eos_token_id,\n",
    "  \t)\n",
    "\n",
    "\t# Decode output & append to outptu list\n",
    "\toutput_text = tokenizer.decode(outputs[0], skip_special_tokens=True)[inp_txt_len + 2:]\n",
    "\tresult = {'input': item['text'], 'output_gen': output_text, 'output_actual': item['output']}\n",
    "\tresults.append(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Quest:\n",
      "Title: A gift for the queen\n",
      "Objective: Bring a gift to the queen\n",
      "Tasks: \n",
      " Arrive at the blue palace.\n",
      " Give the queen the gift\n",
      "\n",
      "\n",
      "### End\n"
     ]
    }
   ],
   "source": [
    "print(results[3]['output_gen'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_kg\n"
     ]
    }
   ],
   "source": [
    "print(TRAIN_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_file = os.path.join(base_dir, 'outputs', f'{TRAIN_TYPE}_{KG_DEPTH}_results.jsonl')\n",
    "with open(out_file, 'w') as outfile:\n",
    "\tfor result in results:\n",
    "\t\tjson.dump(result, outfile)\n",
    "\t\toutfile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/manish/thesis-implementations/quest_generation/llama2/outputs/text_kg_results.jsonl\n"
     ]
    }
   ],
   "source": [
    "print(out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(val_dataset[0]['attention_mask']) == len(val_dataset[0]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_prompt(background_info, plots):\n",
    "    text = ''\n",
    "    if background_info:\n",
    "        text += '### Background:\\n\\n'\n",
    "        text += '\\n'.join(background_info) + '\\n\\n'\n",
    "    text += '### Plots:\\n\\n'\n",
    "    text += \"\\n\".join(plots) + '\\n'\n",
    "    text += '\\n### Quest:\\n'\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Background:\n",
      "\n",
      "Delilah is a character. Delilah is present in Brigmore Manor. Delilah is the leader of the Brigmore Witches.\n",
      "Brigmore Manor is a location. Brigmore Manor is connected to Empire of the Isles. Brigmore Manor is a lair for the city's outlaws.\n",
      "Empire of the Isles is a location. Empire of the Isles is connected to Brigmore Manor. Empire of the Isles is a kingdom.\n",
      "\n",
      "### Plots:\n",
      "\n",
      "Delilah and her coven are planning something that threatens everyone across the Empire of the Isles.\n",
      "Delilah must be stopped\n",
      "\n",
      "### Quest:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bg_info = [\n",
    "    # '',\n",
    "\t'Delilah is a character. Delilah is present in Brigmore Manor. Delilah is the leader of the Brigmore Witches.',\n",
    "\t'Brigmore Manor is a location. Brigmore Manor is connected to Empire of the Isles. Brigmore Manor is a lair for the city\\'s outlaws.',\n",
    " \t'Empire of the Isles is a location. Empire of the Isles is connected to Brigmore Manor. Empire of the Isles is a kingdom.',\n",
    "]\n",
    "plots = [\n",
    "\t'Delilah and her coven are planning something that threatens everyone across the Empire of the Isles.',\n",
    " \t'Delilah must be stopped'\n",
    "\t# 'Infiltrate the ruins of Brigmore Manor and stop Delilah'\n",
    "]\n",
    "\n",
    "str_inp = create_input_prompt(bg_info, plots)\n",
    "print(str_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_inp = tokenizer(out['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1014.00 MiB. GPU 0 has a total capacty of 15.68 GiB of which 139.38 MiB is free. Process 1846 has 239.88 MiB memory in use. Including non-PyTorch memory, this process has 14.76 GiB memory in use. Of the allocated memory 12.05 GiB is allocated by PyTorch, and 2.43 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m/home/manish/thesis-implementations/quest_generation/llama2/finetune-llama2.ipynb Cell 56\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/manish/thesis-implementations/quest_generation/llama2/finetune-llama2.ipynb#Y102sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcuda:0\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/manish/thesis-implementations/quest_generation/llama2/finetune-llama2.ipynb#Y102sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# Get answer\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/manish/thesis-implementations/quest_generation/llama2/finetune-llama2.ipynb#Y102sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# (Adjust max_new_tokens variable as you wish (maximum number of tokens the model can generate to answer the input))\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/manish/thesis-implementations/quest_generation/llama2/finetune-llama2.ipynb#Y102sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m outputs \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/manish/thesis-implementations/quest_generation/llama2/finetune-llama2.ipynb#Y102sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \tinput_ids\u001b[39m=\u001b[39;49minp\u001b[39m.\u001b[39;49mto(device), \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/manish/thesis-implementations/quest_generation/llama2/finetune-llama2.ipynb#Y102sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \tattention_mask\u001b[39m=\u001b[39;49mattn_mask, \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/manish/thesis-implementations/quest_generation/llama2/finetune-llama2.ipynb#Y102sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \tmax_new_tokens\u001b[39m=\u001b[39;49m\u001b[39m250\u001b[39;49m, \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/manish/thesis-implementations/quest_generation/llama2/finetune-llama2.ipynb#Y102sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \teos_token_id\u001b[39m=\u001b[39;49mtokenizer(\u001b[39m'\u001b[39;49m\u001b[39m### End\u001b[39;49m\u001b[39m'\u001b[39;49m)[\u001b[39m'\u001b[39;49m\u001b[39minput_ids\u001b[39;49m\u001b[39m'\u001b[39;49m], \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/manish/thesis-implementations/quest_generation/llama2/finetune-llama2.ipynb#Y102sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \tpad_token_id\u001b[39m=\u001b[39;49mtokenizer\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/manish/thesis-implementations/quest_generation/llama2/finetune-llama2.ipynb#Y102sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/llama2/lib/python3.10/site-packages/peft/peft_model.py:1022\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.generate\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   1020\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_model\u001b[39m.\u001b[39mgeneration_config \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgeneration_config\n\u001b[1;32m   1021\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1022\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbase_model\u001b[39m.\u001b[39;49mgenerate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1023\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m   1024\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_model\u001b[39m.\u001b[39mprepare_inputs_for_generation \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_model_prepare_inputs_for_generation\n",
      "File \u001b[0;32m~/miniconda3/envs/llama2/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/llama2/lib/python3.10/site-packages/transformers/generation/utils.py:1704\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1696\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1697\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1698\u001b[0m         expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1699\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1700\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1701\u001b[0m     )\n\u001b[1;32m   1703\u001b[0m     \u001b[39m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1704\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msample(\n\u001b[1;32m   1705\u001b[0m         input_ids,\n\u001b[1;32m   1706\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   1707\u001b[0m         logits_warper\u001b[39m=\u001b[39;49mlogits_warper,\n\u001b[1;32m   1708\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   1709\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m   1710\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m   1711\u001b[0m         output_scores\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_scores,\n\u001b[1;32m   1712\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   1713\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1714\u001b[0m         streamer\u001b[39m=\u001b[39;49mstreamer,\n\u001b[1;32m   1715\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1716\u001b[0m     )\n\u001b[1;32m   1718\u001b[0m \u001b[39melif\u001b[39;00m generation_mode \u001b[39m==\u001b[39m GenerationMode\u001b[39m.\u001b[39mBEAM_SEARCH:\n\u001b[1;32m   1719\u001b[0m     \u001b[39m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   1720\u001b[0m     beam_scorer \u001b[39m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   1721\u001b[0m         batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m   1722\u001b[0m         num_beams\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1727\u001b[0m         max_length\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mmax_length,\n\u001b[1;32m   1728\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/llama2/lib/python3.10/site-packages/transformers/generation/utils.py:2786\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2783\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2785\u001b[0m \u001b[39m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2786\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\n\u001b[1;32m   2787\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs,\n\u001b[1;32m   2788\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   2789\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   2790\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   2791\u001b[0m )\n\u001b[1;32m   2793\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2794\u001b[0m     \u001b[39mcontinue\u001b[39;00m  \u001b[39m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama2/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/llama2/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama2/lib/python3.10/site-packages/accelerate/hooks.py:164\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    163\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39;49m_old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/llama2/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:1041\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1038\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m   1040\u001b[0m \u001b[39m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1041\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\n\u001b[1;32m   1042\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m   1043\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1044\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1045\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1046\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1047\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1048\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1049\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1050\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1051\u001b[0m )\n\u001b[1;32m   1053\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1054\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mpretraining_tp \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/llama2/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/llama2/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama2/lib/python3.10/site-packages/accelerate/hooks.py:164\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    163\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39;49m_old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/llama2/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:928\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    924\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    925\u001b[0m         create_custom_forward(decoder_layer), hidden_states, attention_mask, position_ids\n\u001b[1;32m    926\u001b[0m     )\n\u001b[1;32m    927\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 928\u001b[0m     layer_outputs \u001b[39m=\u001b[39m decoder_layer(\n\u001b[1;32m    929\u001b[0m         hidden_states,\n\u001b[1;32m    930\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    931\u001b[0m         position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    932\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m    933\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    934\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    935\u001b[0m         padding_mask\u001b[39m=\u001b[39;49mpadding_mask,\n\u001b[1;32m    936\u001b[0m     )\n\u001b[1;32m    938\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    940\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniconda3/envs/llama2/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/llama2/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama2/lib/python3.10/site-packages/accelerate/hooks.py:164\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    163\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39;49m_old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/llama2/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:640\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, padding_mask)\u001b[0m\n\u001b[1;32m    637\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    639\u001b[0m \u001b[39m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 640\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attn(\n\u001b[1;32m    641\u001b[0m     hidden_states\u001b[39m=\u001b[39;49mhidden_states,\n\u001b[1;32m    642\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    643\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    644\u001b[0m     past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m    645\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    646\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    647\u001b[0m     padding_mask\u001b[39m=\u001b[39;49mpadding_mask,\n\u001b[1;32m    648\u001b[0m )\n\u001b[1;32m    649\u001b[0m hidden_states \u001b[39m=\u001b[39m residual \u001b[39m+\u001b[39m hidden_states\n\u001b[1;32m    651\u001b[0m \u001b[39m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama2/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/llama2/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama2/lib/python3.10/site-packages/accelerate/hooks.py:164\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    163\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39;49m_old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/llama2/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:387\u001b[0m, in \u001b[0;36mLlamaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, padding_mask)\u001b[0m\n\u001b[1;32m    384\u001b[0m     attn_weights \u001b[39m=\u001b[39m attn_weights \u001b[39m+\u001b[39m attention_mask\n\u001b[1;32m    386\u001b[0m \u001b[39m# upcast attention to fp32\u001b[39;00m\n\u001b[0;32m--> 387\u001b[0m attn_weights \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mfunctional\u001b[39m.\u001b[39;49msoftmax(attn_weights, dim\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mfloat32)\u001b[39m.\u001b[39mto(query_states\u001b[39m.\u001b[39mdtype)\n\u001b[1;32m    388\u001b[0m attn_output \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmatmul(attn_weights, value_states)\n\u001b[1;32m    390\u001b[0m \u001b[39mif\u001b[39;00m attn_output\u001b[39m.\u001b[39msize() \u001b[39m!=\u001b[39m (bsz, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads, q_len, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_dim):\n",
      "File \u001b[0;32m~/miniconda3/envs/llama2/lib/python3.10/site-packages/torch/nn/functional.py:1858\u001b[0m, in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1856\u001b[0m     ret \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msoftmax(dim)\n\u001b[1;32m   1857\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1858\u001b[0m     ret \u001b[39m=\u001b[39m \u001b[39minput\u001b[39;49m\u001b[39m.\u001b[39;49msoftmax(dim, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[1;32m   1859\u001b[0m \u001b[39mreturn\u001b[39;00m ret\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1014.00 MiB. GPU 0 has a total capacty of 15.68 GiB of which 139.38 MiB is free. Process 1846 has 239.88 MiB memory in use. Including non-PyTorch memory, this process has 14.76 GiB memory in use. Of the allocated memory 12.05 GiB is allocated by PyTorch, and 2.43 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "inp = torch.tensor([token_inp['input_ids']])\n",
    "attn_mask = torch.tensor([token_inp['attention_mask']])\n",
    "\n",
    "# Specify device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Get answer\n",
    "# (Adjust max_new_tokens variable as you wish (maximum number of tokens the model can generate to answer the input))\n",
    "outputs = model.generate(\n",
    "\tinput_ids=inp.to(device), \n",
    "\tattention_mask=attn_mask, \n",
    "\tmax_new_tokens=250, \n",
    "\teos_token_id=tokenizer('### End')['input_ids'], \n",
    "\tpad_token_id=tokenizer.eos_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Background:\n",
      "Gilgondorin is a character. Gilgondorin is a keeper of local legends. Gilgondorin is present in Niben Bay. \n",
      "Niben Bay is a location. Niben Bay is a sea bay. Niben Bay is connected to Bawnwatch Camp. \n",
      "Bawnwatch Camp is a location. Bawnwatch Camp is a camp on the shore of Niben Bay. \n",
      "Watchman is a character. Watchman is a harmless, sad ghost of a sailor. Watchman is present in Niben Bay. \n",
      "\n",
      "\n",
      "### Plots:\n",
      "the player asked Gilgondorin about the Watchman\n",
      "Gilgondorin pinpoints where the Watchman appears on the player's map\n",
      "\n",
      "The quest related to the above information is as follows:\n",
      "\n",
      "### Quest:\n",
      "Title: A watchful eye\n",
      "Objective: Talk to the watchman\n",
      "Tasks: \n",
      " Travel to bawnwatch camp.\n",
      " Find the watchman\n",
      "\n",
      "\n",
      "### End\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Background:\n",
      "\n",
      "\n",
      "\n",
      "### Plots:\n",
      "\n",
      "Delilah and her coven are planning something that threatens everyone across the Empire of the Isles.\n",
      "Infiltrate the ruins of Brigmore Manor and stop Delilah\n",
      "\n",
      "### Quest:\n",
      "\n",
      "Title: The brigmore conspiracy\n",
      "Objective: Infiltrate the ruins of brigmore manor and stop delilah\n",
      "Tasks: \n",
      " Find your way to the ruins of brigmore manor\n",
      "\n",
      "\n",
      "### End\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(base_dir, model_dir, 'results', model_name, f'{TRAIN_TYPE}_{KG_DEPTH}/results.txt'), 'w') as outfile:\n",
    "\tfor result in results:\n",
    "\t\toutfile.write(f'{result}\\n{\"-\"*40}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama2",
   "language": "python",
   "name": "llama2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
