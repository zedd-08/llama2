{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning llama2 models - from turorial\n",
    "\n",
    "[Tutorial](https://blog.ovhcloud.com/fine-tuning-llama-2-models-using-a-single-gpu-qlora-and-ai-notebooks/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning LLaMA 2 Models using a single GPU, QLoRA and AI Notebooks\n",
    "\n",
    "*This tutorial walks through the process of fine-tuning [LLaMA 2](https://ai.meta.com/llama/) models, providing step-by-step instructions.*\n",
    "\n",
    "*All the code related to this article is available in our dedicated [GitHub repository](https://github.com/ovh/ai-training-examples/blob/main/notebooks/natural-language-processing/llm/miniconda/llama2-fine-tuning/llama_2_finetuning.ipynb).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "On July 18, 2023, [Meta](https://about.meta.com/) released LLaMA 2, the latest version of their **Large Language Model** (LLM).\n",
    "\n",
    "Trained between January 2023 and July 2023 on 2 trillion tokens, these new models outperforms other LLMs on many benchmarks, including reasoning, coding, proficiency, and knowledge tests. This release comes in different flavors, with parameter sizes of **[7B](https://huggingface.co/meta-llama/Llama-2-7b-hf)**, **[13B](https://huggingface.co/meta-llama/Llama-2-13b-hf)** and a mind-blowing **[70B](https://huggingface.co/meta-llama/Llama-2-70b-hf)**. Models are intended for free for both commercial and research use in English.\n",
    "\n",
    "To suit every text generation needed and fine-tune these models, we will use [QLoRA](https://arxiv.org/abs/2305.14314) (Efficient Finetuning of Quantized LLMs), a highly efficient fine-tuning technique that involves quantizing a pretrained LLM to just 4 bits and adding small “Low-Rank Adapters”. This unique approach allows for fine-tuning LLMs **using just a single GPU**! This technique is supported by the [PEFT](https://huggingface.co/docs/peft/) library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Python environment\n",
    "The following libraries are used for this method (`requirements.txt` file):\n",
    "\n",
    "```\n",
    "torch\n",
    "accelerate @ git+https://github.com/huggingface/accelerate.git\n",
    "bitsandbytes\n",
    "datasets==2.13.1\n",
    "transformers @ git+https://github.com/huggingface/transformers.git\n",
    "peft @ git+https://github.com/huggingface/peft.git\n",
    "trl @ git+https://github.com/lvwerra/trl.git\n",
    "scipy\n",
    "```\n",
    "\n",
    "Then install and import the installed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bitsandbytes as bnb\n",
    "from datasets import load_dataset\n",
    "from functools import partial\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, AutoPeftModelForCausalLM\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed, Trainer, TrainingArguments, BitsAndBytesConfig, DataCollatorForLanguageModeling, Trainer, TrainingArguments, pipeline\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download LLaMA 2 model\n",
    "As mentioned before, LLaMA 2 models come in different flavors which are 7B, 13B, and 70B. Your choice can be influenced by your computational resources. Indeed, larger models require more resources, memory, processing power, and training time.\n",
    "\n",
    "To download the model you have been granted access to, **make sure you are logged in to the Hugging Face model hub**. As mentioned in the requirements step, you need to use the `huggingface-cli` login command.\n",
    "\n",
    "The following function will help us to download the model and its tokenizer. It requires a bitsandbytes configuration that we will define later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '/home/manish/thesis-implementations/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name, bnb_config):\n",
    "    n_gpus = torch.cuda.device_count()\n",
    "    max_memory = f'{12288}MB'\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\", # dispatch efficiently the model on the available ressources\n",
    "        max_memory = {i: max_memory for i in range(n_gpus)},\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True)\n",
    "\n",
    "    # Needed for LLaMA tokenizer\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quest Dataset\n",
    "\n",
    "Load the qust dataset for training and create prompts accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "dataset_path = os.path.join(base_dir, 'quest_generation/llama2/data')\n",
    "train_file = 'train.jsonl'\n",
    "val_file = 'val.jsonl'\n",
    "data_files = {\n",
    "\t\"train\": train_file, \n",
    "\t\"val\": val_file\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_quests = ''\n",
    "# with open(os.path.join(base_dir, 'data/VartinenFormatted/all_quests.json')) as json_file:\n",
    "# \tall_quests = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "\n",
    "# random.shuffle(all_quests)\n",
    "\n",
    "# full_size = len(all_quests)\n",
    "# train_size = int(full_size * 0.9)\n",
    "\n",
    "# train_set = all_quests[:train_size]\n",
    "# val_set = all_quests[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for out_file, qarr in zip([train_file, val_file], [train_set, val_set]):\n",
    "# \twith open(out_file, 'w') as outfile:\n",
    "# \t\tfor entry in qarr:\n",
    "# \t\t\tjson.dump(entry, outfile)\n",
    "# \t\t\toutfile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %mv ./*.jsonl data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/manish/.cache/huggingface/datasets/json/data-6359e290ba54d2fa/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "909779deff6744e4a2375817d7288bfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(dataset_path, data_files=data_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset['train']\n",
    "val_dataset = dataset['val']\n",
    "\n",
    "del dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a bitsandbytes configuration and load the model and tokenizer\n",
    "This will allow us to load our LLM in 4 bits. This way, we can divide the used memory by 4 and import the model on smaller devices. We choose to apply bfloat16 compute data type and nested quantization for memory-saving purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bnb_config():\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    )\n",
    "\n",
    "    return bnb_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To leverage the LoRa method, we need to wrap the model as a PeftModel.\n",
    "\n",
    "To do this, we need to implement a [LoRa configuration](https://huggingface.co/docs/peft/conceptual_guides/lora):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_peft_config(modules):\n",
    "    \"\"\"\n",
    "    Create Parameter-Efficient Fine-Tuning config for your model\n",
    "    :param modules: Names of the modules to apply Lora to\n",
    "    \"\"\"\n",
    "    config = LoraConfig(\n",
    "        r=16,  # dimension of the updated matrices\n",
    "        lora_alpha=64,  # parameter for scaling\n",
    "        target_modules=modules,\n",
    "        lora_dropout=0.1,  # dropout probability for layers\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff35d8828da0424bbc855ebc17b63d43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/manish/miniconda3/envs/llama2/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:655: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n"
     ]
    }
   ],
   "source": [
    "# Load model from HF with user's token and with bitsandbytes config\n",
    "model_name = \"models/meta-llama/llama-2-13b-hf\" \n",
    "bnb_config = create_bnb_config()\n",
    "model, tokenizer = load_model(model_name, bnb_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS_TOKEN = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt_formats_with_kg(input):\n",
    "    \"\"\"\n",
    "    Format various fields of the input quest data ('plots', 'kb', 'quest')\n",
    "    Then concatenate them using two newline characters \n",
    "    :param input: input dictionary\n",
    "    \"\"\"\n",
    "\n",
    "    BACKGROUND = \"### Background:\"\n",
    "    PLOTS_KEY = \"### Plots:\"\n",
    "    INTRO_BLURB = \"The quest related to the above information is as follows.\"\n",
    "    QUEST = \"### Quest:\"\n",
    "    END_KEY = \"### End\"\n",
    "    \n",
    "\n",
    "    blurb = f\"{INTRO_BLURB}\"  # add intro blurb - model system instruction\n",
    "\n",
    "    background = ''  # add background - knowledge graph as text\n",
    "    for kb in input['kbs']:\n",
    "        entity = kb['name']\n",
    "        desc = kb['description']\n",
    "        e_type = kb['type']\n",
    "        relations = kb['relations']\n",
    "        background += f'{entity} is a {e_type}. '\n",
    "        if entity != desc:\n",
    "            background+= f'{entity} is a {desc}. '\n",
    "        for rel in relations:\n",
    "            background += f' {entity} is {rel[0]} {rel[1]}.'\n",
    "        background += '\\n'\n",
    "    background = f\"{BACKGROUND}\\n{background}\"\n",
    "    plots_str = '\\n'.join(input['plots'])\n",
    "    plots = f\"{PLOTS_KEY}\\n{plots_str}\"  # add plots - key plot points\n",
    "    \n",
    "    quest_str = ''\n",
    "    for k,v in input['quest'].items():\n",
    "        if k == 'description':\n",
    "            continue\n",
    "        if k == 'tasks':\n",
    "            value = '\\n ' + '\\n '.join(np.char.capitalize(v[:-1]))\n",
    "        else:\n",
    "            value = v.capitalize()\n",
    "        quest_str += f'{k.capitalize()}: {value}\\n' \n",
    "    quest = f\"{QUEST}\\n{quest_str}\"  # add quest output\n",
    "    \n",
    "    end = f\"{END_KEY}\"  # add end key\n",
    "    \n",
    "    parts = [part for part in [background, plots, blurb, quest, end] if part]\n",
    "\n",
    "    formatted_prompt = \"\\n\\n\".join(parts)\n",
    "    input['text'] = formatted_prompt + f'\\n{EOS_TOKEN}'\n",
    "\n",
    "    return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt_formats_val_with_kg(input):\n",
    "    \"\"\"\n",
    "    Format various fields of the input quest data ('plots', 'kb', 'quest')\n",
    "    Then concatenate them using two newline characters \n",
    "    :param input: input dictionary\n",
    "    \"\"\"\n",
    "\n",
    "    BACKGROUND = \"### Background:\"\n",
    "    PLOTS_KEY = \"### Plots:\"\n",
    "    INTRO_BLURB = \"The quest related to the above information is as follows.\"\n",
    "    QUEST = \"### Quest:\"\n",
    "    END_KEY = \"### End\"\n",
    "    \n",
    "\n",
    "    blurb = f\"{INTRO_BLURB}\"  # add intro blurb - model system instruction\n",
    "\n",
    "    background = ''  # add background - knowledge graph as text\n",
    "    for kb in input['kbs']:\n",
    "        entity = kb['name']\n",
    "        desc = kb['description']\n",
    "        e_type = kb['type']\n",
    "        relations = kb['relations']\n",
    "        background += f'{entity} is a {e_type}. '\n",
    "        if entity != desc:\n",
    "            background+= f'{entity} is a {desc}. '\n",
    "        for rel in relations:\n",
    "            background += f' {entity} is {rel[0]} {rel[1]}.'\n",
    "        background += '\\n'\n",
    "    background = f\"{BACKGROUND}\\n{background}\"\n",
    "    plots_str = '\\n'.join(input['plots'])\n",
    "    plots = f\"{PLOTS_KEY}\\n{plots_str}\"  # add plots - key plot points\n",
    "    \n",
    "    quest_str = ''\n",
    "    for k,v in input['quest'].items():\n",
    "        if k == 'description':\n",
    "            continue\n",
    "        if k == 'tasks':\n",
    "            value = '\\n ' + '\\n '.join(np.char.capitalize(v[:-1]))\n",
    "        else:\n",
    "            value = v.capitalize()\n",
    "        quest_str += f'{k.capitalize()}: {value}\\n' \n",
    "    quest = f\"{QUEST}\\n{quest_str}\"  # add quest output\n",
    "    \n",
    "    end = f\"{END_KEY}\"  # add end key\n",
    "    \n",
    "    parts_p = [part for part in [background, plots, blurb] if part]\n",
    "    parts_o = [part for part in [quest, end] if part]\n",
    "    \n",
    "    formatted_prompt = \"\\n\\n\".join(parts_p)\n",
    "    formatted_output = \"\\n\\n\".join(parts_o)\n",
    "    input['text'] = formatted_prompt\n",
    "    input['output'] = formatted_output + f'\\n{EOS_TOKEN}'\n",
    "\n",
    "    return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt_formats_without_kg(input):\n",
    "    \"\"\"\n",
    "    Format various fields of the input quest data ('plots', 'quest')\n",
    "    Then concatenate them using two newline characters \n",
    "    :param input: input dictionary\n",
    "    \"\"\"\n",
    "\n",
    "    PLOTS_KEY = \"### Plots:\"\n",
    "    INTRO_BLURB = \"The quest related to the above information is as follows.\"\n",
    "    QUEST = \"### Quest:\"\n",
    "    END_KEY = \"### End\"\n",
    "    \n",
    "    plots_str = '\\n'.join(input['plots'])\n",
    "    quest_str = ''\n",
    "\n",
    "    blurb = f\"{INTRO_BLURB}\"  # add intro blurb - model system instruction\n",
    "    plots = f\"{PLOTS_KEY}\\n{plots_str}\"  # add plots - key plot points\n",
    "    for k,v in input['quest'].items():\n",
    "        if k == 'description':\n",
    "            continue\n",
    "        if k == 'tasks':\n",
    "            value = '\\n ' + '\\n '.join(np.char.capitalize(v[:-1]))\n",
    "        else:\n",
    "            value = v.capitalize()\n",
    "        quest_str += f'{k.capitalize()}: {value}\\n' \n",
    "    quest = f\"{QUEST}\\n{quest_str}\"  # add quest output\n",
    "    end = f\"{END_KEY}\"  # add end key\n",
    "    \n",
    "    parts = [part for part in [plots, blurb, quest, end] if part]\n",
    "\n",
    "    formatted_prompt = \"\\n\\n\".join(parts)\n",
    "    input['text'] = formatted_prompt + f'\\n{EOS_TOKEN}'\n",
    "\n",
    "    return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt_formats_val_without_kg(input):\n",
    "    \"\"\"\n",
    "    Format various fields of the input quest data ('plots', 'quest')\n",
    "    Then concatenate them using two newline characters \n",
    "    :param input: input dictionary\n",
    "    \"\"\"\n",
    "\n",
    "    PLOTS_KEY = \"### Plots:\"\n",
    "    INTRO_BLURB = \"The quest related to the above information is as follows.\"\n",
    "    QUEST = \"### Quest:\"\n",
    "    END_KEY = \"### End\"\n",
    "    \n",
    "    plots_str = '\\n'.join(input['plots'])\n",
    "    quest_str = ''\n",
    "\n",
    "    blurb = f\"{INTRO_BLURB}\"  # add intro blurb - model system instruction\n",
    "    plots = f\"{PLOTS_KEY}\\n{plots_str}\"  # add plots - key plot points\n",
    "    for k,v in input['quest'].items():\n",
    "        if k == 'description':\n",
    "            continue\n",
    "        if k == 'tasks':\n",
    "            value = '\\n ' + '\\n '.join(np.char.capitalize(v[:-1]))\n",
    "        else:\n",
    "            value = v.capitalize()\n",
    "        quest_str += f'{k.capitalize()}: {value}\\n' \n",
    "    quest = f\"{QUEST}\\n{quest_str}\"  # add quest output\n",
    "    end = f\"{END_KEY}\"  # add end key\n",
    "    \n",
    "    \n",
    "    parts_p = [part for part in [plots, blurb] if part]\n",
    "    parts_o = [part for part in [quest, end] if part]\n",
    "\n",
    "    formatted_prompt = \"\\n\\n\".join(parts_p)\n",
    "    formatted_output = \"\\n\\n\".join(parts_o)\n",
    "    input['text'] = formatted_prompt\n",
    "    input['output'] = formatted_output + f'\\n{EOS_TOKEN}'\n",
    "\n",
    "    return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py\n",
    "def get_max_length(model):\n",
    "    max_length = None\n",
    "    for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n",
    "        max_length = getattr(model.config, length_setting, None)\n",
    "        if max_length:\n",
    "            print(f\"Found max lenth: {max_length}\")\n",
    "            break\n",
    "    if not max_length:\n",
    "        max_length = 1024\n",
    "        print(f\"Using default max length: {max_length}\")\n",
    "    return max_length\n",
    "\n",
    "\n",
    "def preprocess_batch(batch, tokenizer, max_length):\n",
    "    \"\"\"\n",
    "    Tokenizing a batch\n",
    "    \"\"\"\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "\n",
    "# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py\n",
    "def preprocess_dataset(tokenizer: AutoTokenizer, max_length: int, dataset: str, include_kg: bool = True):\n",
    "    \"\"\"Format & tokenize it so it is ready for training\n",
    "    :param tokenizer (AutoTokenizer): Model Tokenizer\n",
    "    :param max_length (int): Maximum number of tokens to emit from tokenizer\n",
    "    :param include_kg (bool): Whether to include knowledge graph in the prompt\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add prompt to each sample\n",
    "    print(\"Preprocessing dataset...\")\n",
    "    dataset = dataset.map(create_prompt_formats_with_kg if include_kg else create_prompt_formats_without_kg)#, batched=True)\n",
    "    \n",
    "    # Apply preprocessing to each batch of the dataset & and remove 'instruction', 'context', 'response', 'category' fields\n",
    "    _preprocessing_function = partial(preprocess_batch, max_length=max_length, tokenizer=tokenizer)\n",
    "    dataset = dataset.map(\n",
    "        _preprocessing_function,\n",
    "        batched=True,\n",
    "        remove_columns=[\"id\", \"game\", \"kbs\", \"plots\", \"quest\"],\n",
    "    )\n",
    "\n",
    "    # Filter out samples that have input_ids exceeding max_length\n",
    "    dataset = dataset.filter(lambda sample: len(sample[\"input_ids\"]) < max_length)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py\n",
    "def preprocess_val_dataset(tokenizer: AutoTokenizer, max_length: int, dataset: str, include_kg: bool = True):\n",
    "    \"\"\"Format & tokenize it so it is ready for training\n",
    "    :param tokenizer (AutoTokenizer): Model Tokenizer\n",
    "    :param max_length (int): Maximum number of tokens to emit from tokenizer\n",
    "    :param include_kg (bool): Whether to include knowledge graph in the prompt\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add prompt to each sample\n",
    "    print(\"Preprocessing dataset...\")\n",
    "    dataset = dataset.map(create_prompt_formats_val_with_kg if include_kg else create_prompt_formats_val_without_kg)#, batched=True)\n",
    "    \n",
    "    # Apply preprocessing to each batch of the dataset & and remove 'instruction', 'context', 'response', 'category' fields\n",
    "    _preprocessing_function = partial(preprocess_batch, max_length=max_length, tokenizer=tokenizer)\n",
    "    dataset = dataset.map(\n",
    "        _preprocessing_function,\n",
    "        batched=True,\n",
    "        remove_columns=[\"id\", \"game\", \"kbs\", \"plots\", \"quest\"],\n",
    "    )\n",
    "\n",
    "    # Filter out samples that have input_ids exceeding max_length\n",
    "    dataset = dataset.filter(lambda sample: len(sample[\"input_ids\"]) < max_length)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will use the **model tokenizer to process these prompts into tokenized ones**.\n",
    "\n",
    "The goal is to create input sequences of uniform length (which are suitable for fine-tuning the language model because it maximizes efficiency and minimize computational overhead), that must not exceed the model’s maximum token limit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previous function needs the target modules to update the necessary matrices. The following function will get them for our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOURCE https://github.com/artidoro/qlora/blob/main/qlora.py\n",
    "\n",
    "def find_all_linear_names(model):\n",
    "    cls = bnb.nn.Linear4bit #if args.bits == 4 else (bnb.nn.Linear8bitLt if args.bits == 8 else torch.nn.Linear)\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "\n",
    "    if 'lm_head' in lora_module_names:  # needed for 16-bit\n",
    "        lora_module_names.remove('lm_head')\n",
    "    return list(lora_module_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once everything is set up and the base model is prepared, we can use the `print_trainable_parameters()` helper function to see how many trainable parameters are in the model. We expect the LoRa model to have fewer trainable parameters compared to the original one, since we want to perform fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model, use_4bit=False):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        num_params = param.numel()\n",
    "        # if using DS Zero 3 and the weights are initialized empty\n",
    "        if num_params == 0 and hasattr(param, \"ds_numel\"):\n",
    "            num_params = param.ds_numel\n",
    "\n",
    "        all_param += num_params\n",
    "        if param.requires_grad:\n",
    "            trainable_params += num_params\n",
    "    if use_4bit:\n",
    "        trainable_params /= 2\n",
    "    print(\n",
    "        f\"all params: {all_param:,d} || trainable params: {trainable_params:,d} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "Now that everything is ready, we can pre-process our dataset and load our model using the set configurations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Preprocess dataset\n",
    "# max_length = get_max_length(model)\n",
    "# train_dataset = preprocess_dataset(tokenizer, max_length, train_dataset)\n",
    "# val_dataset = preprocess_val_dataset(tokenizer, max_length, val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/manish/.cache/huggingface/datasets/json/data-6359e290ba54d2fa/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-493dfaefeaeff728.arrow\n",
      "Loading cached processed dataset at /home/manish/.cache/huggingface/datasets/json/data-6359e290ba54d2fa/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-06078d78d729b5c4.arrow\n",
      "Loading cached processed dataset at /home/manish/.cache/huggingface/datasets/json/data-6359e290ba54d2fa/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-eabef810a6db0ed4.arrow\n",
      "Loading cached processed dataset at /home/manish/.cache/huggingface/datasets/json/data-6359e290ba54d2fa/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a9968558bff97f93.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found max lenth: 4096\n",
      "Preprocessing dataset...\n",
      "Preprocessing dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e1e0992ca3d4b12a09b234078e1d6e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/77 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50504fed2aaa49ed9a1eef1ee730155b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/77 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Preprocess dataset for no KG inputs\n",
    "max_length = get_max_length(model)\n",
    "train_dataset = preprocess_dataset(tokenizer, max_length, train_dataset, include_kg=False)\n",
    "val_dataset = preprocess_val_dataset(tokenizer, max_length, val_dataset, include_kg=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all params: 6,734,566,400 || trainable params: 62,586,880 || trainable%: 0.9293379303528732\n",
      "torch.float32 390681600 0.05801139624965313\n",
      "torch.uint8 6343884800 0.9419886037503469\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20/20 01:08, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.838100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.705400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.002800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.828700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.417600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.402400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.307400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.221300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.699600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.546900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.575200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.293600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.371500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.364600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.727900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.348200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.448400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.601000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.567400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.539100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =       0.12\n",
      "  total_flos               =   679948GF\n",
      "  train_loss               =     1.9404\n",
      "  train_runtime            = 0:01:13.07\n",
      "  train_samples_per_second =      1.095\n",
      "  train_steps_per_second   =      0.274\n",
      "{'train_runtime': 73.0789, 'train_samples_per_second': 1.095, 'train_steps_per_second': 0.274, 'total_flos': 730088800358400.0, 'train_loss': 1.9403517186641692, 'epoch': 0.12}\n",
      "Saving last checkpoint of the model...\n"
     ]
    }
   ],
   "source": [
    "def train(model, tokenizer, dataset, output_dir):\n",
    "    # Apply preprocessing to the model to prepare it by\n",
    "    # 1 - Enabling gradient checkpointing to reduce memory usage during fine-tuning\n",
    "    model.gradient_checkpointing_enable()\n",
    "\n",
    "    # 2 - Using the prepare_model_for_kbit_training method from PEFT\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "    # Get lora module names\n",
    "    modules = find_all_linear_names(model)\n",
    "\n",
    "    # Create PEFT config for these modules and wrap the model to PEFT\n",
    "    peft_config = create_peft_config(modules)\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    \n",
    "    # Print information about the percentage of trainable parameters\n",
    "    print_trainable_parameters(model)\n",
    "    \n",
    "    # Training parameters\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        train_dataset=dataset,\n",
    "        args=TrainingArguments(\n",
    "            per_device_train_batch_size=1,\n",
    "            gradient_accumulation_steps=4,\n",
    "            warmup_steps=10,\n",
    "            max_steps=20,\n",
    "            learning_rate=2e-4,\n",
    "            fp16=True,\n",
    "            logging_steps=1,\n",
    "            output_dir=\"outputs\",\n",
    "            optim=\"paged_adamw_8bit\",\n",
    "        ),\n",
    "        data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "    )\n",
    "    \n",
    "    model.config.use_cache = False  # re-enable for inference to speed up predictions for similar inputs\n",
    "    \n",
    "    ### SOURCE https://github.com/artidoro/qlora/blob/main/qlora.py\n",
    "    # Verifying the datatypes before training\n",
    "    \n",
    "    dtypes = {}\n",
    "    for _, p in model.named_parameters():\n",
    "        dtype = p.dtype\n",
    "        if dtype not in dtypes: dtypes[dtype] = 0\n",
    "        dtypes[dtype] += p.numel()\n",
    "    total = 0\n",
    "    for k, v in dtypes.items(): total+= v\n",
    "    for k, v in dtypes.items():\n",
    "        print(k, v, v/total)\n",
    "     \n",
    "    do_train = True\n",
    "    \n",
    "    # Launch training\n",
    "    print(\"Training...\")\n",
    "    \n",
    "    if do_train:\n",
    "        train_result = trainer.train()\n",
    "        metrics = train_result.metrics\n",
    "        trainer.log_metrics(\"train\", metrics)\n",
    "        trainer.save_metrics(\"train\", metrics)\n",
    "        trainer.save_state()\n",
    "        print(metrics)    \n",
    "    \n",
    "    ###\n",
    "    \n",
    "    # Saving model\n",
    "    print(\"Saving last checkpoint of the model...\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    trainer.model.save_pretrained(output_dir)\n",
    "    \n",
    "    # Free memory for merging weights\n",
    "    del model\n",
    "    del trainer\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    \n",
    "output_dir = \"models/results/llama-2-13b/final_checkpoint\"\n",
    "train(model, tokenizer, train_dataset, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*If you prefer to have a number of epochs (entire training dataset will be passed through the model) instead of a number of training steps (forward and backward passes through the model with one batch of data), you can replace the `max_steps` argument by `num_train_epochs`.*\n",
    "\n",
    "To later load and use the model for inference, we have used the `trainer.model.save_pretrained(output_dir)` function, which saves the fine-tuned model’s weights, configuration, and tokenizer files.\n",
    "\n",
    "Unfortunately, it is possible that the latest weights are not the best. To solve this problem, you can implement a `EarlyStoppingCallback`, from transformers, during your fine-tuning. This will enable you to regularly test your model on the validation set, if you have one, and keep only the best weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import Dataset\n",
    "\n",
    "# class ListDataset(Dataset):\n",
    "#      def __init__(self, original_list):\n",
    "#         self.original_list = original_list\n",
    "#      def __len__(self):\n",
    "#         return len(self.original_list)\n",
    "\n",
    "#      def __getitem__(self, i):\n",
    "#         return self.original_list[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm.auto import tqdm\n",
    "# # Run text generation pipeline with our new model\n",
    "# prompts = ListDataset(val_dataset['text'])\n",
    "# pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=500)\n",
    "# results = []\n",
    "\n",
    "# for out in tqdm(pipe(prompts)):\n",
    "#     results.append(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('results.jsonl', 'w') as outfile:\n",
    "# \tfor result in results:\n",
    "# \t\tresult[0]['generated_text'] = result[0]['generated_text'].split('### END')[0].strip()\n",
    "# \t\tjson.dump(result[0], outfile)\n",
    "# \t\toutfile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b7f740fdebb4e49bd65f8dfb7f4bf20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/77 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/home/manish/miniconda3/envs/llama2/lib/python3.10/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "results = []\n",
    "\n",
    "for item in tqdm(val_dataset):\n",
    "\t# Specify input\n",
    "\tinp = torch.tensor([item['input_ids']])\n",
    "\tattn_mask = torch.tensor([item['attention_mask']])\n",
    "\n",
    "\t# Specify device\n",
    "\tdevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\t# Get answer\n",
    "\t# (Adjust max_new_tokens variable as you wish (maximum number of tokens the model can generate to answer the input))\n",
    "\toutputs = model.generate(input_ids=inp.to(device), attention_mask=attn_mask, max_new_tokens=250, eos_token_id=tokenizer('### End')['input_ids'], pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "\t# Decode output & append to outptu list\n",
    "\tresults.append(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('outputs/results.txt', 'w') as outfile:\n",
    "\tfor result in results:\n",
    "\t\toutfile.write(f'{result}\\n{\"-\"*40}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge weights\n",
    "Once we have our fine-tuned weights, we can build our fine-tuned model and save it to a new directory, with its associated tokenizer. By performing these steps, we can have a memory-efficient fine-tuned model and tokenizer ready for inference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del pipe\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"models/results/llama-2-13b/final_checkpoint\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_model = load_model(model_name, bnb_config)\n",
    "base_model = model\n",
    "\n",
    "from peft import PeftModel    \n",
    "\n",
    "model = PeftModel.from_pretrained(base_model, output_dir)\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "output_merged_dir = \"models/results/llama-2-13b/final_merged_checkpoint\"\n",
    "os.makedirs(output_merged_dir, exist_ok=True)\n",
    "model.save_pretrained(output_merged_dir, safe_serialization=True)\n",
    "\n",
    "# save tokenizer for easy inference\n",
    "# model_name = \"models/meta-llama/llama-2-13b-hf\" \n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# tokenizer.save_pretrained(output_merged_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocess validation dataset\n",
    "max_length = get_max_length(model)\n",
    "val_dataset = preprocess_val_dataset(tokenizer, max_length, val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "from ctransformers import AutoModelForCausalLM, LlamaTokenizer, LlamaForCausalLM\n",
    "import os\n",
    "import sys\n",
    "from typing import List\n",
    " \n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    prepare_model_for_int8_training,\n",
    ")\n",
    " \n",
    "import fire\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    " \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    " \n",
    "%matplotlib inline\n",
    "sns.set(rc={'figure.figsize':(10, 7)})\n",
    "sns.set(rc={'figure.dpi':100})\n",
    "sns.set(style='white', palette='muted', font_scale=1.2)\n",
    " \n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AutoModelForCausalLM.from_pretrained(\"llama2/TheBloke/Llama-2-13B-Ensemble-v5-GGUF\", \n",
    "\t\t\t\t\t\t\t\t\t\t   model_file=\"llama-2-13b-ensemble-v5.Q5_0.gguf\", \n",
    "\t\t\t\t\t\t\t\t\t\t   model_type=\"llama\", \n",
    "\t\t\t\t\t\t\t\t\t\t   gpu_layers=1000000000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
