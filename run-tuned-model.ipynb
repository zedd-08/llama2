{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the tuned models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Python environment\n",
    "The following libraries are used for this method (`requirements.txt` file):\n",
    "\n",
    "```\n",
    "torch\n",
    "accelerate @ git+https://github.com/huggingface/accelerate.git\n",
    "bitsandbytes\n",
    "datasets==2.13.1\n",
    "transformers @ git+https://github.com/huggingface/transformers.git\n",
    "peft @ git+https://github.com/huggingface/peft.git\n",
    "trl @ git+https://github.com/lvwerra/trl.git\n",
    "scipy\n",
    "```\n",
    "\n",
    "Then install and import the installed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bitsandbytes as bnb\n",
    "from datasets import load_dataset\n",
    "from functools import partial\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, AutoPeftModelForCausalLM\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, BitsAndBytesConfig, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the tuned model into memory\n",
    "\n",
    "The fine-tuned models require the base model as well as the adaptors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "base_dir = '/home/manish/thesis-implementations/quest_generation/llama2/'\n",
    "model_name = 'meta-llama/llama-2-13b-hf'\n",
    "model_dir = 'models'\n",
    "model_path = os.path.join(base_dir, model_dir, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model type and KG depth\n",
    "\n",
    "To try varying KG depths and model types, we can change the following variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN_TYPE = 'no_kg'\n",
    "TRAIN_TYPE = 'text_kg'\n",
    "# TRAIN_TYPE = 'tree_kg'\n",
    "KG_DEPTH = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/manish/thesis-implementations/quest_generation/llama2/models/results/meta-llama/llama-2-13b-hf/text_kg True\n"
     ]
    }
   ],
   "source": [
    "output_dir = os.path.join(base_dir, model_dir, 'results', model_name, f'{TRAIN_TYPE}')\n",
    "print(output_dir, os.path.exists(output_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/manish/thesis-implementations/quest_generation/llama2/models/meta-llama/llama-2-13b-hf True\n"
     ]
    }
   ],
   "source": [
    "print(model_path, os.path.exists(model_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a bitsandbytes configuration and load the model and tokenizer\n",
    "This will allow us to load our LLM in 4 bits. This way, we can divide the used memory by 4 and import the model on smaller devices. We choose to apply bfloat16 compute data type and nested quantization for memory-saving purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bnb_config():\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    )\n",
    "\n",
    "    return bnb_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokenizer(model_name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    # Needed for LLaMA tokenizer\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d13b757a64647bbb098075245dce25b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_pretrained_model(model_path, bnb_config, base_model_path):\n",
    "    n_gpus = torch.cuda.device_count()\n",
    "    max_memory = f'{12288}MB'\n",
    "\n",
    "    model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        max_memory = {i: max_memory for i in range(n_gpus)},\n",
    "    )\n",
    "    \n",
    "    return model, load_tokenizer(base_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = create_bnb_config()\n",
    "model, tokenizer = load_pretrained_model(output_dir, bnb_config, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quest Dataset\n",
    "\n",
    "Load the qust dataset for training and create prompts accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "\n",
    "dataset_path = os.path.join(base_dir, 'data')\n",
    "train_file = 'train.jsonl'\n",
    "val_file = 'val.jsonl'\n",
    "data_files = {\n",
    "\t\"train\": train_file, \n",
    "\t\"val\": val_file\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS_TOKEN = tokenizer.eos_token\n",
    "PAD_TOKEN = tokenizer.pad_token\n",
    "BOS_TOKEN = tokenizer.bos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the full game KG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_game = {\n",
    "    'TESO': 'TESOblivion_KG.gml',\n",
    "    'TESS': 'TESSkyrim_KG.gml',\n",
    "    'TL2': 'Torchlight2_KG.gml',\n",
    "    'MC': 'Minecraft_KG.gml',\n",
    "    'BG1': 'BaldursGate1_KG.gml',\n",
    "    'BG2': 'BaldursGate2_KG.gml'\n",
    "}\n",
    "\n",
    "kg_map = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_base_dir = '/home/manish/thesis-implementations/data/VartinenFormatted/KGs'\n",
    "\n",
    "for gid, gname in map_game.items():\n",
    "    kg_path = os.path.join(kg_base_dir, map_game[gid])\n",
    "    kg = nx.read_gml(kg_path)\n",
    "    kg_map[gid] = kg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_generation_prompt_formats(input):\n",
    "    \"\"\"\n",
    "    Format various fields of the input quest data ('plots', 'kb', 'quest')\n",
    "    Then concatenate them using two newline characters\n",
    "    :param input: input dictionary\n",
    "    \"\"\"\n",
    "\n",
    "    BACKGROUND = \"### Background:\"\n",
    "    PLOTS_KEY = \"### Plots:\"\n",
    "    INTRO_BLURB = \"The quest related to the above information is as follows:\"\n",
    "    QUEST = \"### Quest:\"\n",
    "    END_KEY = \"### End\"\n",
    "    \n",
    "\n",
    "    blurb = f\"{INTRO_BLURB}\"  # add intro blurb - model system instruction\n",
    "\n",
    "    # add background - only if knowledge graph as text\n",
    "    # create sentence tree from plots using kg - only if knowledge graph as tree\n",
    "    background = ''\n",
    "    plots_str = '\\n'.join(input['plots'])\n",
    "    plots = f\"{PLOTS_KEY}\\n{plots_str}\"  # add plots - key plot points\n",
    "    \n",
    "    if TRAIN_TYPE == 'text_kg':\n",
    "        completed_rels = []\n",
    "        completed_nodes = []\n",
    "        \n",
    "        for kb in input['kbs']:\n",
    "            entity = kb['name']\n",
    "            e_desc = kb['description']\n",
    "            e_type = kb['type']\n",
    "            e_relations = kb['relations']\n",
    "            \n",
    "            background += f'{entity} is a {e_type}. '\n",
    "            \n",
    "            # add depth of information from KG\n",
    "            # KG_DEPTH <= 0 will add all information\n",
    "            # KG_DEPTH = 1 will only add description\n",
    "            # KG_DEPTH > 1 will randomly add (KG_DEPTH - 1) number\n",
    "            # of relations and the description\n",
    "            if KG_DEPTH > 0:\n",
    "                if entity != e_desc:\n",
    "                    background+= f'{entity} is {e_desc}. '\n",
    "                depth = KG_DEPTH - 1\n",
    "                if depth > len(e_relations):\n",
    "                    depth = len(e_relations)\n",
    "                relations = random.sample(e_relations, depth)\n",
    "            else:\n",
    "                relations = e_relations\n",
    "                \n",
    "            for rel in relations:\n",
    "                background += f'{entity} is {rel[0]} {rel[1]}. '\n",
    "                completed_rels.append((entity, rel[1]))\n",
    "            completed_nodes.append(entity)                     \n",
    "            background += '\\n'\n",
    "        \n",
    "        if KG_DEPTH > 0:\n",
    "            kg = kg_map[input['game']]        \n",
    "            all_nodes = kg.nodes(data=True)\n",
    "            for node in all_nodes:\n",
    "                entity = node[0]\n",
    "                if entity.lower() in plots.lower():\n",
    "                    edges = list(nx.dfs_edges(kg, source=entity, depth_limit=KG_DEPTH))\n",
    "                    for ent1, ent2 in edges:\n",
    "                        if (ent1, ent2) in completed_rels or (ent2, ent1) in completed_rels:\n",
    "                            continue\n",
    "                        e1_type = all_nodes[ent1]['type']\n",
    "                        e1_desc = all_nodes[ent1]['description']\n",
    "                        e2_type = all_nodes[ent2]['type']\n",
    "                        e2_desc = all_nodes[ent2]['description']\n",
    "                        \n",
    "                        if ent1 not in completed_nodes:\n",
    "                            background += f'{ent1} is a {e1_type}. '\n",
    "                            if e1_desc != ent1:\n",
    "                                background += f'{ent1} is {e1_desc}. '\n",
    "                            completed_nodes.append(ent1)\n",
    "                            background += '\\n'\n",
    "                        \n",
    "                        if ent2 not in completed_nodes:\n",
    "                            background += f'{ent2} is a {e2_type}. '\n",
    "                            if e2_desc != ent2:\n",
    "                                background += f'{ent2} is {e2_desc}. '\n",
    "                            completed_nodes.append(ent2)\n",
    "                            background += '\\n'\n",
    "                        \n",
    "                        rel = kg[ent1][ent2]['label']\n",
    "                        if rel == 'connected to':\n",
    "                            background += f'{ent1} is {rel} {ent2}. '\n",
    "                        if rel == 'present in':\n",
    "                            if e1_type == 'location':\n",
    "                                background += f'{ent2} is {rel} {ent2}. '\n",
    "                            else:\n",
    "                                background += f'{ent1} is {rel} {ent2}. '\n",
    "                        if rel == 'held by':\n",
    "                            if e1_type == 'character':\n",
    "                                background += f'{ent2} is {rel} {ent1}. '\n",
    "                            else:\n",
    "                                background += f'{ent1} is {rel} {ent2}. '\n",
    "                        background += '\\n'\n",
    "                        completed_rels.append((ent1, ent2))\n",
    "                \n",
    "        background = f\"{BACKGROUND}\\n{background}\"\n",
    "        \n",
    "    # add concatenated quest text\n",
    "    quest_str = ''\n",
    "    for k,v in input['quest'].items():\n",
    "        if k == 'description':\n",
    "            continue\n",
    "        if k == 'tasks':\n",
    "            value = '\\n ' + '\\n '.join(np.char.capitalize(v[:-1]))\n",
    "        else:\n",
    "            value = v.capitalize()\n",
    "        quest_str += f'{k.capitalize()}: {value}\\n' \n",
    "    quest = f\"{QUEST}\\n{quest_str}\"  # add quest output\n",
    "    \n",
    "    end = f\"{END_KEY}\"  # add end key\n",
    "    \n",
    "    if TRAIN_TYPE in ['no_kg', 'tree_kg']:\n",
    "        parts_p = [part for part in [plots, blurb] if part]\n",
    "    else:\n",
    "        parts_p = [part for part in [background, plots, blurb] if part]\n",
    "\n",
    "    parts_o = [part for part in [quest, end] if part]\n",
    "    \n",
    "    formatted_prompt = \"\\n\\n\".join(parts_p)\n",
    "    formatted_output = \"\\n\\n\".join(parts_o)\n",
    "    input['text'] = formatted_prompt\n",
    "    input['output'] = formatted_output\n",
    "\n",
    "    return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py\n",
    "def get_max_length(model):\n",
    "    max_length = None\n",
    "    for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n",
    "        max_length = getattr(model.config, length_setting, None)\n",
    "        if max_length:\n",
    "            print(f\"Found max lenth: {max_length}\")\n",
    "            break\n",
    "    if not max_length:\n",
    "        max_length = 1024\n",
    "        print(f\"Using default max length: {max_length}\")\n",
    "    return max_length\n",
    "\n",
    "\n",
    "def preprocess_batch(batch, tokenizer, max_length):\n",
    "    \"\"\"\n",
    "    Tokenizing a batch\n",
    "    \"\"\"\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py\n",
    "def preprocess_val_dataset(tokenizer: AutoTokenizer, max_length: int, dataset: str, include_kg: bool = True):\n",
    "    \"\"\"Format & tokenize it so it is ready for training\n",
    "    :param tokenizer (AutoTokenizer): Model Tokenizer\n",
    "    :param max_length (int): Maximum number of tokens to emit from tokenizer\n",
    "    :param include_kg (bool): Whether to include knowledge graph in the prompt\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add prompt to each sample\n",
    "    print(\"Preprocessing dataset...\")\n",
    "    dataset = dataset.map(create_generation_prompt_formats)#, batched=True)\n",
    "    \n",
    "    # Apply preprocessing to each batch of the dataset & and remove 'instruction', 'context', 'response', 'category' fields\n",
    "    _preprocessing_function = partial(preprocess_batch, max_length=max_length, tokenizer=tokenizer)\n",
    "    dataset = dataset.map(\n",
    "        _preprocessing_function,\n",
    "        batched=True,\n",
    "        remove_columns=[\"id\", \"game\", \"kbs\", \"plots\", \"quest\"],\n",
    "    )\n",
    "\n",
    "    # Filter out samples that have input_ids exceeding max_length\n",
    "    dataset = dataset.filter(lambda sample: len(sample[\"input_ids\"]) < max_length)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/manish/.cache/huggingface/datasets/json/data-6359e290ba54d2fa/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e56cb5db983c4055a753d2d882f4a69f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(dataset_path, data_files=data_files)\n",
    "train_dataset = dataset['train']\n",
    "val_dataset = dataset['val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found max lenth: 4096\n",
      "Preprocessing dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06757d948dfb400c966345f8c97182d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/77 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dae06090d114da592a4659f84805d28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/77 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9043ada43cd044868b4c3794717d959d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/77 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Preprocess dataset\n",
    "max_length = get_max_length(model)\n",
    "val_dataset = preprocess_val_dataset(tokenizer, max_length, val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "Get the outputs from the processed inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5109"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "results = []\n",
    "\n",
    "for item in tqdm(val_dataset):\n",
    "\tinp_txt_len = len(item['text'])\n",
    "\tprint(inp_txt_len)\n",
    "\t# Specify input\n",
    "\tinp = torch.tensor([item['input_ids']])\n",
    "\tattn_mask = torch.tensor([item['attention_mask']])\n",
    "\n",
    "\t# Specify device\n",
    "\tdevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\t# Get answer\n",
    "\t# (Adjust max_new_tokens variable as you wish (maximum number of tokens the model can generate to answer the input))\n",
    "\toutputs = model.generate(\n",
    "\t\tinput_ids=inp.to(device), \n",
    "\t\tattention_mask=attn_mask, \n",
    "\t\tmax_new_tokens=250, \n",
    "\t\teos_token_id=tokenizer('### End')['input_ids'], \n",
    "\t\tpad_token_id=tokenizer.eos_token_id,\n",
    "  \t)\n",
    "\n",
    "\t# Decode output & append to outptu list\n",
    "\toutput_text = tokenizer.decode(outputs[0], skip_special_tokens=True)[inp_txt_len + 2:]\n",
    "\tresult = {'input': item['text'], 'output_gen': output_text, 'output_actual': item['output']}\n",
    "\tresults.append(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_kg\n"
     ]
    }
   ],
   "source": [
    "print(TRAIN_TYPE, f'KG Depth = {KG_DEPTH}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/manish/thesis-implementations/quest_generation/llama2/outputs/text_kg_results.jsonl\n"
     ]
    }
   ],
   "source": [
    "out_file = os.path.join(base_dir, 'outputs', f'{TRAIN_TYPE}_{KG_DEPTH}_results.jsonl')\n",
    "print('Saving results  @ ', out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(out_file, 'w') as outfile:\n",
    "\tfor result in results:\n",
    "\t\tjson.dump(result, outfile)\n",
    "\t\toutfile.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_prompt(background_info, plots):\n",
    "    text = ''\n",
    "    if background_info:\n",
    "        text += '### Background:\\n\\n'\n",
    "        text += '\\n'.join(background_info) + '\\n\\n'\n",
    "    text += '### Plots:\\n\\n'\n",
    "    text += \"\\n\".join(plots) + '\\n'\n",
    "    text += '\\n### Quest:\\n'\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Background:\n",
      "\n",
      "Delilah is a character. Delilah is present in Brigmore Manor. Delilah is the leader of the Brigmore Witches.\n",
      "Brigmore Manor is a location. Brigmore Manor is connected to Empire of the Isles. Brigmore Manor is a lair for the city's outlaws.\n",
      "Empire of the Isles is a location. Empire of the Isles is connected to Brigmore Manor. Empire of the Isles is a kingdom.\n",
      "\n",
      "### Plots:\n",
      "\n",
      "Delilah and her coven are planning something that threatens everyone across the Empire of the Isles.\n",
      "Delilah must be stopped\n",
      "\n",
      "### Quest:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bg_info = [\n",
    "    # '',\n",
    "\t'Delilah is a character. Delilah is present in Brigmore Manor. Delilah is the leader of the Brigmore Witches.',\n",
    "\t'Brigmore Manor is a location. Brigmore Manor is connected to Empire of the Isles. Brigmore Manor is a lair for the city\\'s outlaws.',\n",
    " \t'Empire of the Isles is a location. Empire of the Isles is connected to Brigmore Manor. Empire of the Isles is a kingdom.',\n",
    "]\n",
    "plots = [\n",
    "\t'Delilah and her coven are planning something that threatens everyone across the Empire of the Isles.',\n",
    " \t'Delilah must be stopped'\n",
    "\t# 'Infiltrate the ruins of Brigmore Manor and stop Delilah'\n",
    "]\n",
    "\n",
    "str_inp = create_input_prompt(bg_info, plots)\n",
    "print(str_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_inp = tokenizer(str_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.tensor([token_inp['input_ids']])\n",
    "attn_mask = torch.tensor([token_inp['attention_mask']])\n",
    "\n",
    "# Specify device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Get answer\n",
    "# (Adjust max_new_tokens variable as you wish (maximum number of tokens the model can generate to answer the input))\n",
    "outputs = model.generate(\n",
    "\tinput_ids=inp.to(device), \n",
    "\tattention_mask=attn_mask, \n",
    "\tmax_new_tokens=250, \n",
    "\teos_token_id=tokenizer('### End')['input_ids'], \n",
    "\tpad_token_id=tokenizer.eos_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Background:\n",
      "Gilgondorin is a character. Gilgondorin is a keeper of local legends. Gilgondorin is present in Niben Bay. \n",
      "Niben Bay is a location. Niben Bay is a sea bay. Niben Bay is connected to Bawnwatch Camp. \n",
      "Bawnwatch Camp is a location. Bawnwatch Camp is a camp on the shore of Niben Bay. \n",
      "Watchman is a character. Watchman is a harmless, sad ghost of a sailor. Watchman is present in Niben Bay. \n",
      "\n",
      "\n",
      "### Plots:\n",
      "the player asked Gilgondorin about the Watchman\n",
      "Gilgondorin pinpoints where the Watchman appears on the player's map\n",
      "\n",
      "The quest related to the above information is as follows:\n",
      "\n",
      "### Quest:\n",
      "Title: A watchful eye\n",
      "Objective: Talk to the watchman\n",
      "Tasks: \n",
      " Travel to bawnwatch camp.\n",
      " Find the watchman\n",
      "\n",
      "\n",
      "### End\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Background:\n",
      "\n",
      "\n",
      "\n",
      "### Plots:\n",
      "\n",
      "Delilah and her coven are planning something that threatens everyone across the Empire of the Isles.\n",
      "Infiltrate the ruins of Brigmore Manor and stop Delilah\n",
      "\n",
      "### Quest:\n",
      "\n",
      "Title: The brigmore conspiracy\n",
      "Objective: Infiltrate the ruins of brigmore manor and stop delilah\n",
      "Tasks: \n",
      " Find your way to the ruins of brigmore manor\n",
      "\n",
      "\n",
      "### End\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(base_dir, model_dir, 'results', model_name, f'{TRAIN_TYPE}_{KG_DEPTH}/results.txt'), 'w') as outfile:\n",
    "\tfor result in results:\n",
    "\t\toutfile.write(f'{result}\\n{\"-\"*40}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama2",
   "language": "python",
   "name": "llama2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
